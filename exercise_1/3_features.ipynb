{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Features\n",
    "We have seen that we can achieve reasonable performance on an image classification task by training a linear classifier on the pixels of the input image. In this exercise we will show that we can improve our classification performance by training linear classifiers not on raw pixels but on features that are computed from the raw pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from exercise_code.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "Similar to previous exercises, we will load CIFAR-10 data from disk. However, we don't apply our previous preprocessing steps, as we want to extract custom features later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (48000, 32, 32, 3)\n",
      "Train labels shape:  (48000,)\n",
      "Validation data shape:  (1000, 32, 32, 3)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 32, 32, 3)\n",
      "Test labels shape:  (1000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9f1c3cd550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from exercise_code.data_utils import load_CIFAR10\n",
    "from exercise_code.vis_utils import visualize_cifar10\n",
    "\n",
    "def get_CIFAR10_data(num_training=48000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for feature extraction and training.\n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'datasets/'\n",
    "    X, y = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "\n",
    "    # Our training set will be the first num_train points from the original\n",
    "    # training set.\n",
    "    mask = range(num_training)\n",
    "    X_train = X[mask]\n",
    "    y_train = y[mask]\n",
    "    \n",
    "    # Our validation set will be num_validation points from the original\n",
    "    # training set.\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X[mask]\n",
    "    y_val = y[mask]\n",
    "    \n",
    "    # We use a small subset of the training set as our test set.\n",
    "    mask = range(num_training + num_validation, num_training + num_validation + num_test)\n",
    "    X_test = X[mask]\n",
    "    y_test = y[mask]\n",
    "\n",
    "    return X, y, X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_raw, y_raw, X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "\n",
    "# visualize raw data\n",
    "visualize_cifar10(X_raw, y_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features\n",
    "For each image we will compute a Histogram of Oriented\n",
    "Gradients (HOG) as well as a color histogram using the hue channel in HSV\n",
    "color space. We form our final feature vector for each image by concatenating\n",
    "the HOG and color histogram feature vectors.\n",
    "\n",
    "Roughly speaking, HOG should capture the texture of the image while ignoring\n",
    "color information, and the color histogram represents the color of the input\n",
    "image while ignoring texture. As a result, we expect that using both together\n",
    "ought to work better than using either alone.\n",
    "\n",
    "The `hog_feature` and `color_histogram_hsv` functions both operate on a single\n",
    "image and return a feature vector for that image. The extract_features\n",
    "function takes a set of images and a list of feature functions and evaluates\n",
    "each feature function on each image, storing the results in a matrix where\n",
    "each column is the concatenation of all feature vectors for a single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done extracting features for 1000/48000 images\n",
      "Done extracting features for 2000/48000 images\n",
      "Done extracting features for 3000/48000 images\n",
      "Done extracting features for 4000/48000 images\n",
      "Done extracting features for 5000/48000 images\n",
      "Done extracting features for 6000/48000 images\n",
      "Done extracting features for 7000/48000 images\n",
      "Done extracting features for 8000/48000 images\n",
      "Done extracting features for 9000/48000 images\n",
      "Done extracting features for 10000/48000 images\n",
      "Done extracting features for 11000/48000 images\n",
      "Done extracting features for 12000/48000 images\n",
      "Done extracting features for 13000/48000 images\n",
      "Done extracting features for 14000/48000 images\n",
      "Done extracting features for 15000/48000 images\n",
      "Done extracting features for 16000/48000 images\n",
      "Done extracting features for 17000/48000 images\n",
      "Done extracting features for 18000/48000 images\n",
      "Done extracting features for 19000/48000 images\n",
      "Done extracting features for 20000/48000 images\n",
      "Done extracting features for 21000/48000 images\n",
      "Done extracting features for 22000/48000 images\n",
      "Done extracting features for 23000/48000 images\n",
      "Done extracting features for 24000/48000 images\n",
      "Done extracting features for 25000/48000 images\n",
      "Done extracting features for 26000/48000 images\n",
      "Done extracting features for 27000/48000 images\n",
      "Done extracting features for 28000/48000 images\n",
      "Done extracting features for 29000/48000 images\n",
      "Done extracting features for 30000/48000 images\n",
      "Done extracting features for 31000/48000 images\n",
      "Done extracting features for 32000/48000 images\n",
      "Done extracting features for 33000/48000 images\n",
      "Done extracting features for 34000/48000 images\n",
      "Done extracting features for 35000/48000 images\n",
      "Done extracting features for 36000/48000 images\n",
      "Done extracting features for 37000/48000 images\n",
      "Done extracting features for 38000/48000 images\n",
      "Done extracting features for 39000/48000 images\n",
      "Done extracting features for 40000/48000 images\n",
      "Done extracting features for 41000/48000 images\n",
      "Done extracting features for 42000/48000 images\n",
      "Done extracting features for 43000/48000 images\n",
      "Done extracting features for 44000/48000 images\n",
      "Done extracting features for 45000/48000 images\n",
      "Done extracting features for 46000/48000 images\n",
      "Done extracting features for 47000/48000 images\n",
      "[[  2.25203125e-01   7.34366504e-01   6.15900296e-02 ...,   9.76562500e-04\n",
      "    1.17187500e-02   2.92968750e-03]\n",
      " [  7.82262035e-02   0.00000000e+00   0.00000000e+00 ...,   1.36718750e-02\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [  6.90613950e+00   2.00441182e+00   2.27071132e+00 ...,   3.90625000e-03\n",
      "    1.46484375e-02   5.66406250e-02]\n",
      " ..., \n",
      " [  2.27914042e+00   1.15161461e+00   9.57307021e-01 ...,   5.37109375e-02\n",
      "    2.24609375e-02   4.00390625e-02]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   1.48437500e-01\n",
      "    1.78710938e-01   3.88671875e-01]\n",
      " [  1.12435374e-01   1.27417331e-01   5.25595380e-01 ...,   1.95312500e-03\n",
      "    1.07421875e-02   3.51562500e-02]]\n"
     ]
    }
   ],
   "source": [
    "from exercise_code.features import *\n",
    "\n",
    "num_color_bins = 10 # Number of bins in the color histogram\n",
    "feature_fns = [hog_feature, lambda img: color_histogram_hsv(img, nbin=num_color_bins)]\n",
    "X_train_feats = extract_features(X_train, feature_fns, verbose=True)\n",
    "print(X_train_feats)\n",
    "X_val_feats = extract_features(X_val, feature_fns)\n",
    "X_test_feats = extract_features(X_test, feature_fns)\n",
    "\n",
    "# Preprocessing: Subtract the mean feature\n",
    "mean_feat = np.mean(X_train_feats, axis=0, keepdims=True)\n",
    "X_train_feats -= mean_feat\n",
    "X_val_feats -= mean_feat\n",
    "X_test_feats -= mean_feat\n",
    "\n",
    "# Preprocessing: Divide by standard deviation. This ensures that each feature\n",
    "# has roughly the same scale.\n",
    "std_feat = np.std(X_train_feats, axis=0, keepdims=True)\n",
    "X_train_feats /= std_feat\n",
    "X_val_feats /= std_feat\n",
    "X_test_feats /= std_feat\n",
    "\n",
    "# Preprocessing: Add a bias dimension\n",
    "X_train_feats = np.hstack([X_train_feats, np.ones((X_train_feats.shape[0], 1))])\n",
    "X_val_feats = np.hstack([X_val_feats, np.ones((X_val_feats.shape[0], 1))])\n",
    "X_test_feats = np.hstack([X_test_feats, np.ones((X_test_feats.shape[0], 1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Softmax classifier on features\n",
    "Using the multiclass Softmax code developed earlier in the assignment, train a Softmax classifier on top of the features extracted above; this should achieve better results than training the classifier directly on top of raw pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 20000: loss 76.405783\n",
      "iteration 100 / 20000: loss 74.938387\n",
      "iteration 200 / 20000: loss 73.499552\n",
      "iteration 300 / 20000: loss 72.089572\n",
      "iteration 400 / 20000: loss 70.708654\n",
      "iteration 500 / 20000: loss 69.352866\n",
      "iteration 600 / 20000: loss 68.025428\n",
      "iteration 700 / 20000: loss 66.724836\n",
      "iteration 800 / 20000: loss 65.447751\n",
      "iteration 900 / 20000: loss 64.198931\n",
      "iteration 1000 / 20000: loss 62.971697\n",
      "iteration 1100 / 20000: loss 61.771295\n",
      "iteration 1200 / 20000: loss 60.592941\n",
      "iteration 1300 / 20000: loss 59.440573\n",
      "iteration 1400 / 20000: loss 58.307605\n",
      "iteration 1500 / 20000: loss 57.198081\n",
      "iteration 1600 / 20000: loss 56.111409\n",
      "iteration 1700 / 20000: loss 55.045322\n",
      "iteration 1800 / 20000: loss 54.002407\n",
      "iteration 1900 / 20000: loss 52.978141\n",
      "iteration 2000 / 20000: loss 51.974489\n",
      "iteration 2100 / 20000: loss 50.989424\n",
      "iteration 2200 / 20000: loss 50.026509\n",
      "iteration 2300 / 20000: loss 49.081029\n",
      "iteration 2400 / 20000: loss 48.155716\n",
      "iteration 2500 / 20000: loss 47.247868\n",
      "iteration 2600 / 20000: loss 46.357749\n",
      "iteration 2700 / 20000: loss 45.484932\n",
      "iteration 2800 / 20000: loss 44.629584\n",
      "iteration 2900 / 20000: loss 43.790796\n",
      "iteration 3000 / 20000: loss 42.970953\n",
      "iteration 3100 / 20000: loss 42.163916\n",
      "iteration 3200 / 20000: loss 41.374409\n",
      "iteration 3300 / 20000: loss 40.600275\n",
      "iteration 3400 / 20000: loss 39.842474\n",
      "iteration 3500 / 20000: loss 39.099070\n",
      "iteration 3600 / 20000: loss 38.370480\n",
      "iteration 3700 / 20000: loss 37.656829\n",
      "iteration 3800 / 20000: loss 36.956770\n",
      "iteration 3900 / 20000: loss 36.272326\n",
      "iteration 4000 / 20000: loss 35.597555\n",
      "iteration 4100 / 20000: loss 34.936949\n",
      "iteration 4200 / 20000: loss 34.291458\n",
      "iteration 4300 / 20000: loss 33.658419\n",
      "iteration 4400 / 20000: loss 33.037590\n",
      "iteration 4500 / 20000: loss 32.430267\n",
      "iteration 4600 / 20000: loss 31.832012\n",
      "iteration 4700 / 20000: loss 31.247404\n",
      "iteration 4800 / 20000: loss 30.675173\n",
      "iteration 4900 / 20000: loss 30.112455\n",
      "iteration 5000 / 20000: loss 29.562360\n",
      "iteration 5100 / 20000: loss 29.021494\n",
      "iteration 5200 / 20000: loss 28.493507\n",
      "iteration 5300 / 20000: loss 27.974684\n",
      "iteration 5400 / 20000: loss 27.466145\n",
      "iteration 5500 / 20000: loss 26.967867\n",
      "iteration 5600 / 20000: loss 26.479704\n",
      "iteration 5700 / 20000: loss 26.001571\n",
      "iteration 5800 / 20000: loss 25.532220\n",
      "iteration 5900 / 20000: loss 25.071870\n",
      "iteration 6000 / 20000: loss 24.620647\n",
      "iteration 6100 / 20000: loss 24.179104\n",
      "iteration 6200 / 20000: loss 23.745490\n",
      "iteration 6300 / 20000: loss 23.321851\n",
      "iteration 6400 / 20000: loss 22.904027\n",
      "iteration 6500 / 20000: loss 22.496099\n",
      "iteration 6600 / 20000: loss 22.097315\n",
      "iteration 6700 / 20000: loss 21.704647\n",
      "iteration 6800 / 20000: loss 21.321079\n",
      "iteration 6900 / 20000: loss 20.943795\n",
      "iteration 7000 / 20000: loss 20.575769\n",
      "iteration 7100 / 20000: loss 20.212722\n",
      "iteration 7200 / 20000: loss 19.858457\n",
      "iteration 7300 / 20000: loss 19.510287\n",
      "iteration 7400 / 20000: loss 19.170216\n",
      "iteration 7500 / 20000: loss 18.836091\n",
      "iteration 7600 / 20000: loss 18.508654\n",
      "iteration 7700 / 20000: loss 18.187113\n",
      "iteration 7800 / 20000: loss 17.873098\n",
      "iteration 7900 / 20000: loss 17.565119\n",
      "iteration 8000 / 20000: loss 17.262162\n",
      "iteration 8100 / 20000: loss 16.966038\n",
      "iteration 8200 / 20000: loss 16.675520\n",
      "iteration 8300 / 20000: loss 16.391055\n",
      "iteration 8400 / 20000: loss 16.112238\n",
      "iteration 8500 / 20000: loss 15.838496\n",
      "iteration 8600 / 20000: loss 15.570504\n",
      "iteration 8700 / 20000: loss 15.308157\n",
      "iteration 8800 / 20000: loss 15.050549\n",
      "iteration 8900 / 20000: loss 14.797634\n",
      "iteration 9000 / 20000: loss 14.550426\n",
      "iteration 9100 / 20000: loss 14.308010\n",
      "iteration 9200 / 20000: loss 14.071184\n",
      "iteration 9300 / 20000: loss 13.837428\n",
      "iteration 9400 / 20000: loss 13.608941\n",
      "iteration 9500 / 20000: loss 13.384530\n",
      "iteration 9600 / 20000: loss 13.165669\n",
      "iteration 9700 / 20000: loss 12.950023\n",
      "iteration 9800 / 20000: loss 12.739818\n",
      "iteration 9900 / 20000: loss 12.532526\n",
      "iteration 10000 / 20000: loss 12.330455\n",
      "iteration 10100 / 20000: loss 12.131961\n",
      "iteration 10200 / 20000: loss 11.937443\n",
      "iteration 10300 / 20000: loss 11.746567\n",
      "iteration 10400 / 20000: loss 11.559042\n",
      "iteration 10500 / 20000: loss 11.376321\n",
      "iteration 10600 / 20000: loss 11.196278\n",
      "iteration 10700 / 20000: loss 11.020453\n",
      "iteration 10800 / 20000: loss 10.847482\n",
      "iteration 10900 / 20000: loss 10.677918\n",
      "iteration 11000 / 20000: loss 10.512315\n",
      "iteration 11100 / 20000: loss 10.350196\n",
      "iteration 11200 / 20000: loss 10.190608\n",
      "iteration 11300 / 20000: loss 10.034415\n",
      "iteration 11400 / 20000: loss 9.881243\n",
      "iteration 11500 / 20000: loss 9.730998\n",
      "iteration 11600 / 20000: loss 9.583778\n",
      "iteration 11700 / 20000: loss 9.440149\n",
      "iteration 11800 / 20000: loss 9.298491\n",
      "iteration 11900 / 20000: loss 9.160102\n",
      "iteration 12000 / 20000: loss 9.024539\n",
      "iteration 12100 / 20000: loss 8.890881\n",
      "iteration 12200 / 20000: loss 8.760936\n",
      "iteration 12300 / 20000: loss 8.632565\n",
      "iteration 12400 / 20000: loss 8.507521\n",
      "iteration 12500 / 20000: loss 8.385058\n",
      "iteration 12600 / 20000: loss 8.264029\n",
      "iteration 12700 / 20000: loss 8.146082\n",
      "iteration 12800 / 20000: loss 8.030715\n",
      "iteration 12900 / 20000: loss 7.916841\n",
      "iteration 13000 / 20000: loss 7.805565\n",
      "iteration 13100 / 20000: loss 7.696985\n",
      "iteration 13200 / 20000: loss 7.590113\n",
      "iteration 13300 / 20000: loss 7.485223\n",
      "iteration 13400 / 20000: loss 7.382600\n",
      "iteration 13500 / 20000: loss 7.282066\n",
      "iteration 13600 / 20000: loss 7.183450\n",
      "iteration 13700 / 20000: loss 7.086924\n",
      "iteration 13800 / 20000: loss 6.991913\n",
      "iteration 13900 / 20000: loss 6.898887\n",
      "iteration 14000 / 20000: loss 6.808151\n",
      "iteration 14100 / 20000: loss 6.718713\n",
      "iteration 14200 / 20000: loss 6.631454\n",
      "iteration 14300 / 20000: loss 6.545615\n",
      "iteration 14400 / 20000: loss 6.461603\n",
      "iteration 14500 / 20000: loss 6.379326\n",
      "iteration 14600 / 20000: loss 6.298411\n",
      "iteration 14700 / 20000: loss 6.219347\n",
      "iteration 14800 / 20000: loss 6.142146\n",
      "iteration 14900 / 20000: loss 6.065766\n",
      "iteration 15000 / 20000: loss 5.991663\n",
      "iteration 15100 / 20000: loss 5.918336\n",
      "iteration 15200 / 20000: loss 5.846461\n",
      "iteration 15300 / 20000: loss 5.776314\n",
      "iteration 15400 / 20000: loss 5.707870\n",
      "iteration 15500 / 20000: loss 5.640320\n",
      "iteration 15600 / 20000: loss 5.573864\n",
      "iteration 15700 / 20000: loss 5.509470\n",
      "iteration 15800 / 20000: loss 5.446112\n",
      "iteration 15900 / 20000: loss 5.383685\n",
      "iteration 16000 / 20000: loss 5.322940\n",
      "iteration 16100 / 20000: loss 5.262987\n",
      "iteration 16200 / 20000: loss 5.204408\n",
      "iteration 16300 / 20000: loss 5.146793\n",
      "iteration 16400 / 20000: loss 5.090530\n",
      "iteration 16500 / 20000: loss 5.035349\n",
      "iteration 16600 / 20000: loss 4.981192\n",
      "iteration 16700 / 20000: loss 4.928193\n",
      "iteration 16800 / 20000: loss 4.876294\n",
      "iteration 16900 / 20000: loss 4.825242\n",
      "iteration 17000 / 20000: loss 4.775314\n",
      "iteration 17100 / 20000: loss 4.726189\n",
      "iteration 17200 / 20000: loss 4.678349\n",
      "iteration 17300 / 20000: loss 4.631434\n",
      "iteration 17400 / 20000: loss 4.585098\n",
      "iteration 17500 / 20000: loss 4.539802\n",
      "iteration 17600 / 20000: loss 4.495498\n",
      "iteration 17700 / 20000: loss 4.452343\n",
      "iteration 17800 / 20000: loss 4.409750\n",
      "iteration 17900 / 20000: loss 4.367789\n",
      "iteration 18000 / 20000: loss 4.326890\n",
      "iteration 18100 / 20000: loss 4.287018\n",
      "iteration 18200 / 20000: loss 4.247645\n",
      "iteration 18300 / 20000: loss 4.209008\n",
      "iteration 18400 / 20000: loss 4.171290\n",
      "iteration 18500 / 20000: loss 4.134454\n",
      "iteration 18600 / 20000: loss 4.097966\n",
      "iteration 18700 / 20000: loss 4.062546\n",
      "iteration 18800 / 20000: loss 4.027597\n",
      "iteration 18900 / 20000: loss 3.993446\n",
      "iteration 19000 / 20000: loss 3.960048\n",
      "iteration 19100 / 20000: loss 3.927062\n",
      "iteration 19200 / 20000: loss 3.894922\n",
      "iteration 19300 / 20000: loss 3.863413\n",
      "iteration 19400 / 20000: loss 3.832808\n",
      "iteration 19500 / 20000: loss 3.802243\n",
      "iteration 19600 / 20000: loss 3.772439\n",
      "iteration 19700 / 20000: loss 3.743598\n",
      "iteration 19800 / 20000: loss 3.714838\n",
      "iteration 19900 / 20000: loss 3.686978\n",
      "training accuracy: 0.110333\n",
      "validation accuracy: 0.108000\n",
      "iteration 0 / 20000: loss 737.791854\n",
      "iteration 100 / 20000: loss 604.408875\n",
      "iteration 200 / 20000: loss 495.217687\n",
      "iteration 300 / 20000: loss 405.825440\n",
      "iteration 400 / 20000: loss 332.647469\n",
      "iteration 500 / 20000: loss 272.737081\n",
      "iteration 600 / 20000: loss 223.694193\n",
      "iteration 700 / 20000: loss 183.544395\n",
      "iteration 800 / 20000: loss 150.676328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 900 / 20000: loss 123.768494\n",
      "iteration 1000 / 20000: loss 101.740888\n",
      "iteration 1100 / 20000: loss 83.707900\n",
      "iteration 1200 / 20000: loss 68.944246\n",
      "iteration 1300 / 20000: loss 56.859139\n",
      "iteration 1400 / 20000: loss 46.964590\n",
      "iteration 1500 / 20000: loss 38.865469\n",
      "iteration 1600 / 20000: loss 32.234634\n",
      "iteration 1700 / 20000: loss 26.806529\n",
      "iteration 1800 / 20000: loss 22.362828\n",
      "iteration 1900 / 20000: loss 18.724512\n",
      "iteration 2000 / 20000: loss 15.746634\n",
      "iteration 2100 / 20000: loss 13.308450\n",
      "iteration 2200 / 20000: loss 11.312380\n",
      "iteration 2300 / 20000: loss 9.678553\n",
      "iteration 2400 / 20000: loss 8.340864\n",
      "iteration 2500 / 20000: loss 7.245807\n",
      "iteration 2600 / 20000: loss 6.349324\n",
      "iteration 2700 / 20000: loss 5.615579\n",
      "iteration 2800 / 20000: loss 5.014737\n",
      "iteration 2900 / 20000: loss 4.522837\n",
      "iteration 3000 / 20000: loss 4.120240\n",
      "iteration 3100 / 20000: loss 3.790599\n",
      "iteration 3200 / 20000: loss 3.520722\n",
      "iteration 3300 / 20000: loss 3.299835\n",
      "iteration 3400 / 20000: loss 3.119002\n",
      "iteration 3500 / 20000: loss 2.970897\n",
      "iteration 3600 / 20000: loss 2.849698\n",
      "iteration 3700 / 20000: loss 2.750502\n",
      "iteration 3800 / 20000: loss 2.669259\n",
      "iteration 3900 / 20000: loss 2.602758\n",
      "iteration 4000 / 20000: loss 2.548317\n",
      "iteration 4100 / 20000: loss 2.503750\n",
      "iteration 4200 / 20000: loss 2.467283\n",
      "iteration 4300 / 20000: loss 2.437411\n",
      "iteration 4400 / 20000: loss 2.412969\n",
      "iteration 4500 / 20000: loss 2.392954\n",
      "iteration 4600 / 20000: loss 2.376553\n",
      "iteration 4700 / 20000: loss 2.363147\n",
      "iteration 4800 / 20000: loss 2.352165\n",
      "iteration 4900 / 20000: loss 2.343156\n",
      "iteration 5000 / 20000: loss 2.335826\n",
      "iteration 5100 / 20000: loss 2.329792\n",
      "iteration 5200 / 20000: loss 2.324852\n",
      "iteration 5300 / 20000: loss 2.320806\n",
      "iteration 5400 / 20000: loss 2.317501\n",
      "iteration 5500 / 20000: loss 2.314806\n",
      "iteration 5600 / 20000: loss 2.312585\n",
      "iteration 5700 / 20000: loss 2.310772\n",
      "iteration 5800 / 20000: loss 2.309287\n",
      "iteration 5900 / 20000: loss 2.308074\n",
      "iteration 6000 / 20000: loss 2.307076\n",
      "iteration 6100 / 20000: loss 2.306263\n",
      "iteration 6200 / 20000: loss 2.305597\n",
      "iteration 6300 / 20000: loss 2.305049\n",
      "iteration 6400 / 20000: loss 2.304605\n",
      "iteration 6500 / 20000: loss 2.304237\n",
      "iteration 6600 / 20000: loss 2.303936\n",
      "iteration 6700 / 20000: loss 2.303689\n",
      "iteration 6800 / 20000: loss 2.303491\n",
      "iteration 6900 / 20000: loss 2.303327\n",
      "iteration 7000 / 20000: loss 2.303192\n",
      "iteration 7100 / 20000: loss 2.303082\n",
      "iteration 7200 / 20000: loss 2.302991\n",
      "iteration 7300 / 20000: loss 2.302917\n",
      "iteration 7400 / 20000: loss 2.302857\n",
      "iteration 7500 / 20000: loss 2.302808\n",
      "iteration 7600 / 20000: loss 2.302767\n",
      "iteration 7700 / 20000: loss 2.302735\n",
      "iteration 7800 / 20000: loss 2.302708\n",
      "iteration 7900 / 20000: loss 2.302685\n",
      "iteration 8000 / 20000: loss 2.302667\n",
      "iteration 8100 / 20000: loss 2.302652\n",
      "iteration 8200 / 20000: loss 2.302640\n",
      "iteration 8300 / 20000: loss 2.302630\n",
      "iteration 8400 / 20000: loss 2.302622\n",
      "iteration 8500 / 20000: loss 2.302615\n",
      "iteration 8600 / 20000: loss 2.302610\n",
      "iteration 8700 / 20000: loss 2.302605\n",
      "iteration 8800 / 20000: loss 2.302601\n",
      "iteration 8900 / 20000: loss 2.302598\n",
      "iteration 9000 / 20000: loss 2.302596\n",
      "iteration 9100 / 20000: loss 2.302594\n",
      "iteration 9200 / 20000: loss 2.302592\n",
      "iteration 9300 / 20000: loss 2.302591\n",
      "iteration 9400 / 20000: loss 2.302590\n",
      "iteration 9500 / 20000: loss 2.302589\n",
      "iteration 9600 / 20000: loss 2.302588\n",
      "iteration 9700 / 20000: loss 2.302588\n",
      "iteration 9800 / 20000: loss 2.302587\n",
      "iteration 9900 / 20000: loss 2.302587\n",
      "iteration 10000 / 20000: loss 2.302586\n",
      "iteration 10100 / 20000: loss 2.302586\n",
      "iteration 10200 / 20000: loss 2.302586\n",
      "iteration 10300 / 20000: loss 2.302586\n",
      "iteration 10400 / 20000: loss 2.302585\n",
      "iteration 10500 / 20000: loss 2.302585\n",
      "iteration 10600 / 20000: loss 2.302585\n",
      "iteration 10700 / 20000: loss 2.302585\n",
      "iteration 10800 / 20000: loss 2.302585\n",
      "iteration 10900 / 20000: loss 2.302585\n",
      "iteration 11000 / 20000: loss 2.302585\n",
      "iteration 11100 / 20000: loss 2.302585\n",
      "iteration 11200 / 20000: loss 2.302585\n",
      "iteration 11300 / 20000: loss 2.302585\n",
      "iteration 11400 / 20000: loss 2.302585\n",
      "iteration 11500 / 20000: loss 2.302585\n",
      "iteration 11600 / 20000: loss 2.302585\n",
      "iteration 11700 / 20000: loss 2.302585\n",
      "iteration 11800 / 20000: loss 2.302585\n",
      "iteration 11900 / 20000: loss 2.302585\n",
      "iteration 12000 / 20000: loss 2.302585\n",
      "iteration 12100 / 20000: loss 2.302585\n",
      "iteration 12200 / 20000: loss 2.302585\n",
      "iteration 12300 / 20000: loss 2.302585\n",
      "iteration 12400 / 20000: loss 2.302585\n",
      "iteration 12500 / 20000: loss 2.302585\n",
      "iteration 12600 / 20000: loss 2.302585\n",
      "iteration 12700 / 20000: loss 2.302585\n",
      "iteration 12800 / 20000: loss 2.302585\n",
      "iteration 12900 / 20000: loss 2.302585\n",
      "iteration 13000 / 20000: loss 2.302585\n",
      "iteration 13100 / 20000: loss 2.302585\n",
      "iteration 13200 / 20000: loss 2.302585\n",
      "iteration 13300 / 20000: loss 2.302585\n",
      "iteration 13400 / 20000: loss 2.302585\n",
      "iteration 13500 / 20000: loss 2.302585\n",
      "iteration 13600 / 20000: loss 2.302585\n",
      "iteration 13700 / 20000: loss 2.302585\n",
      "iteration 13800 / 20000: loss 2.302585\n",
      "iteration 13900 / 20000: loss 2.302585\n",
      "iteration 14000 / 20000: loss 2.302585\n",
      "iteration 14100 / 20000: loss 2.302585\n",
      "iteration 14200 / 20000: loss 2.302585\n",
      "iteration 14300 / 20000: loss 2.302585\n",
      "iteration 14400 / 20000: loss 2.302585\n",
      "iteration 14500 / 20000: loss 2.302585\n",
      "iteration 14600 / 20000: loss 2.302585\n",
      "iteration 14700 / 20000: loss 2.302585\n",
      "iteration 14800 / 20000: loss 2.302585\n",
      "iteration 14900 / 20000: loss 2.302585\n",
      "iteration 15000 / 20000: loss 2.302585\n",
      "iteration 15100 / 20000: loss 2.302585\n",
      "iteration 15200 / 20000: loss 2.302585\n",
      "iteration 15300 / 20000: loss 2.302585\n",
      "iteration 15400 / 20000: loss 2.302585\n",
      "iteration 15500 / 20000: loss 2.302585\n",
      "iteration 15600 / 20000: loss 2.302585\n",
      "iteration 15700 / 20000: loss 2.302585\n",
      "iteration 15800 / 20000: loss 2.302585\n",
      "iteration 15900 / 20000: loss 2.302585\n",
      "iteration 16000 / 20000: loss 2.302585\n",
      "iteration 16100 / 20000: loss 2.302585\n",
      "iteration 16200 / 20000: loss 2.302585\n",
      "iteration 16300 / 20000: loss 2.302585\n",
      "iteration 16400 / 20000: loss 2.302585\n",
      "iteration 16500 / 20000: loss 2.302585\n",
      "iteration 16600 / 20000: loss 2.302585\n",
      "iteration 16700 / 20000: loss 2.302585\n",
      "iteration 16800 / 20000: loss 2.302585\n",
      "iteration 16900 / 20000: loss 2.302585\n",
      "iteration 17000 / 20000: loss 2.302585\n",
      "iteration 17100 / 20000: loss 2.302585\n",
      "iteration 17200 / 20000: loss 2.302585\n",
      "iteration 17300 / 20000: loss 2.302585\n",
      "iteration 17400 / 20000: loss 2.302585\n",
      "iteration 17500 / 20000: loss 2.302585\n",
      "iteration 17600 / 20000: loss 2.302585\n",
      "iteration 17700 / 20000: loss 2.302585\n",
      "iteration 17800 / 20000: loss 2.302585\n",
      "iteration 17900 / 20000: loss 2.302585\n",
      "iteration 18000 / 20000: loss 2.302585\n",
      "iteration 18100 / 20000: loss 2.302585\n",
      "iteration 18200 / 20000: loss 2.302585\n",
      "iteration 18300 / 20000: loss 2.302585\n",
      "iteration 18400 / 20000: loss 2.302585\n",
      "iteration 18500 / 20000: loss 2.302585\n",
      "iteration 18600 / 20000: loss 2.302585\n",
      "iteration 18700 / 20000: loss 2.302585\n",
      "iteration 18800 / 20000: loss 2.302585\n",
      "iteration 18900 / 20000: loss 2.302585\n",
      "iteration 19000 / 20000: loss 2.302585\n",
      "iteration 19100 / 20000: loss 2.302585\n",
      "iteration 19200 / 20000: loss 2.302585\n",
      "iteration 19300 / 20000: loss 2.302585\n",
      "iteration 19400 / 20000: loss 2.302585\n",
      "iteration 19500 / 20000: loss 2.302585\n",
      "iteration 19600 / 20000: loss 2.302585\n",
      "iteration 19700 / 20000: loss 2.302585\n",
      "iteration 19800 / 20000: loss 2.302585\n",
      "iteration 19900 / 20000: loss 2.302585\n",
      "training accuracy: 0.413813\n",
      "validation accuracy: 0.418000\n",
      "iteration 0 / 20000: loss 7835.999937\n",
      "iteration 100 / 20000: loss 1051.858881\n",
      "iteration 200 / 20000: loss 142.921845\n",
      "iteration 300 / 20000: loss 21.142689\n",
      "iteration 400 / 20000: loss 4.826768\n",
      "iteration 500 / 20000: loss 2.640779\n",
      "iteration 600 / 20000: loss 2.347899\n",
      "iteration 700 / 20000: loss 2.308654\n",
      "iteration 800 / 20000: loss 2.303398\n",
      "iteration 900 / 20000: loss 2.302694\n",
      "iteration 1000 / 20000: loss 2.302600\n",
      "iteration 1100 / 20000: loss 2.302587\n",
      "iteration 1200 / 20000: loss 2.302585\n",
      "iteration 1300 / 20000: loss 2.302585\n",
      "iteration 1400 / 20000: loss 2.302585\n",
      "iteration 1500 / 20000: loss 2.302585\n",
      "iteration 1600 / 20000: loss 2.302585\n",
      "iteration 1700 / 20000: loss 2.302585\n",
      "iteration 1800 / 20000: loss 2.302585\n",
      "iteration 1900 / 20000: loss 2.302585\n",
      "iteration 2000 / 20000: loss 2.302585\n",
      "iteration 2100 / 20000: loss 2.302585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2200 / 20000: loss 2.302585\n",
      "iteration 2300 / 20000: loss 2.302585\n",
      "iteration 2400 / 20000: loss 2.302585\n",
      "iteration 2500 / 20000: loss 2.302585\n",
      "iteration 2600 / 20000: loss 2.302585\n",
      "iteration 2700 / 20000: loss 2.302585\n",
      "iteration 2800 / 20000: loss 2.302585\n",
      "iteration 2900 / 20000: loss 2.302585\n",
      "iteration 3000 / 20000: loss 2.302585\n",
      "iteration 3100 / 20000: loss 2.302585\n",
      "iteration 3200 / 20000: loss 2.302585\n",
      "iteration 3300 / 20000: loss 2.302585\n",
      "iteration 3400 / 20000: loss 2.302585\n",
      "iteration 3500 / 20000: loss 2.302585\n",
      "iteration 3600 / 20000: loss 2.302585\n",
      "iteration 3700 / 20000: loss 2.302585\n",
      "iteration 3800 / 20000: loss 2.302585\n",
      "iteration 3900 / 20000: loss 2.302585\n",
      "iteration 4000 / 20000: loss 2.302585\n",
      "iteration 4100 / 20000: loss 2.302585\n",
      "iteration 4200 / 20000: loss 2.302585\n",
      "iteration 4300 / 20000: loss 2.302585\n",
      "iteration 4400 / 20000: loss 2.302585\n",
      "iteration 4500 / 20000: loss 2.302585\n",
      "iteration 4600 / 20000: loss 2.302585\n",
      "iteration 4700 / 20000: loss 2.302585\n",
      "iteration 4800 / 20000: loss 2.302585\n",
      "iteration 4900 / 20000: loss 2.302585\n",
      "iteration 5000 / 20000: loss 2.302585\n",
      "iteration 5100 / 20000: loss 2.302585\n",
      "iteration 5200 / 20000: loss 2.302585\n",
      "iteration 5300 / 20000: loss 2.302585\n",
      "iteration 5400 / 20000: loss 2.302585\n",
      "iteration 5500 / 20000: loss 2.302585\n",
      "iteration 5600 / 20000: loss 2.302585\n",
      "iteration 5700 / 20000: loss 2.302585\n",
      "iteration 5800 / 20000: loss 2.302585\n",
      "iteration 5900 / 20000: loss 2.302585\n",
      "iteration 6000 / 20000: loss 2.302585\n",
      "iteration 6100 / 20000: loss 2.302585\n",
      "iteration 6200 / 20000: loss 2.302585\n",
      "iteration 6300 / 20000: loss 2.302585\n",
      "iteration 6400 / 20000: loss 2.302585\n",
      "iteration 6500 / 20000: loss 2.302585\n",
      "iteration 6600 / 20000: loss 2.302585\n",
      "iteration 6700 / 20000: loss 2.302585\n",
      "iteration 6800 / 20000: loss 2.302585\n",
      "iteration 6900 / 20000: loss 2.302585\n",
      "iteration 7000 / 20000: loss 2.302585\n",
      "iteration 7100 / 20000: loss 2.302585\n",
      "iteration 7200 / 20000: loss 2.302585\n",
      "iteration 7300 / 20000: loss 2.302585\n",
      "iteration 7400 / 20000: loss 2.302585\n",
      "iteration 7500 / 20000: loss 2.302585\n",
      "iteration 7600 / 20000: loss 2.302585\n",
      "iteration 7700 / 20000: loss 2.302585\n",
      "iteration 7800 / 20000: loss 2.302585\n",
      "iteration 7900 / 20000: loss 2.302585\n",
      "iteration 8000 / 20000: loss 2.302585\n",
      "iteration 8100 / 20000: loss 2.302585\n",
      "iteration 8200 / 20000: loss 2.302585\n",
      "iteration 8300 / 20000: loss 2.302585\n",
      "iteration 8400 / 20000: loss 2.302585\n",
      "iteration 8500 / 20000: loss 2.302585\n",
      "iteration 8600 / 20000: loss 2.302585\n",
      "iteration 8700 / 20000: loss 2.302585\n",
      "iteration 8800 / 20000: loss 2.302585\n",
      "iteration 8900 / 20000: loss 2.302585\n",
      "iteration 9000 / 20000: loss 2.302585\n",
      "iteration 9100 / 20000: loss 2.302585\n",
      "iteration 9200 / 20000: loss 2.302585\n",
      "iteration 9300 / 20000: loss 2.302585\n",
      "iteration 9400 / 20000: loss 2.302585\n",
      "iteration 9500 / 20000: loss 2.302585\n",
      "iteration 9600 / 20000: loss 2.302585\n",
      "iteration 9700 / 20000: loss 2.302585\n",
      "iteration 9800 / 20000: loss 2.302585\n",
      "iteration 9900 / 20000: loss 2.302585\n",
      "iteration 10000 / 20000: loss 2.302585\n",
      "iteration 10100 / 20000: loss 2.302585\n",
      "iteration 10200 / 20000: loss 2.302585\n",
      "iteration 10300 / 20000: loss 2.302585\n",
      "iteration 10400 / 20000: loss 2.302585\n",
      "iteration 10500 / 20000: loss 2.302585\n",
      "iteration 10600 / 20000: loss 2.302585\n",
      "iteration 10700 / 20000: loss 2.302585\n",
      "iteration 10800 / 20000: loss 2.302585\n",
      "iteration 10900 / 20000: loss 2.302585\n",
      "iteration 11000 / 20000: loss 2.302585\n",
      "iteration 11100 / 20000: loss 2.302585\n",
      "iteration 11200 / 20000: loss 2.302585\n",
      "iteration 11300 / 20000: loss 2.302585\n",
      "iteration 11400 / 20000: loss 2.302585\n",
      "iteration 11500 / 20000: loss 2.302585\n",
      "iteration 11600 / 20000: loss 2.302585\n",
      "iteration 11700 / 20000: loss 2.302585\n",
      "iteration 11800 / 20000: loss 2.302585\n",
      "iteration 11900 / 20000: loss 2.302585\n",
      "iteration 12000 / 20000: loss 2.302585\n",
      "iteration 12100 / 20000: loss 2.302585\n",
      "iteration 12200 / 20000: loss 2.302585\n",
      "iteration 12300 / 20000: loss 2.302585\n",
      "iteration 12400 / 20000: loss 2.302585\n",
      "iteration 12500 / 20000: loss 2.302585\n",
      "iteration 12600 / 20000: loss 2.302585\n",
      "iteration 12700 / 20000: loss 2.302585\n",
      "iteration 12800 / 20000: loss 2.302585\n",
      "iteration 12900 / 20000: loss 2.302585\n",
      "iteration 13000 / 20000: loss 2.302585\n",
      "iteration 13100 / 20000: loss 2.302585\n",
      "iteration 13200 / 20000: loss 2.302585\n",
      "iteration 13300 / 20000: loss 2.302585\n",
      "iteration 13400 / 20000: loss 2.302585\n",
      "iteration 13500 / 20000: loss 2.302585\n",
      "iteration 13600 / 20000: loss 2.302585\n",
      "iteration 13700 / 20000: loss 2.302585\n",
      "iteration 13800 / 20000: loss 2.302585\n",
      "iteration 13900 / 20000: loss 2.302585\n",
      "iteration 14000 / 20000: loss 2.302585\n",
      "iteration 14100 / 20000: loss 2.302585\n",
      "iteration 14200 / 20000: loss 2.302585\n",
      "iteration 14300 / 20000: loss 2.302585\n",
      "iteration 14400 / 20000: loss 2.302585\n",
      "iteration 14500 / 20000: loss 2.302585\n",
      "iteration 14600 / 20000: loss 2.302585\n",
      "iteration 14700 / 20000: loss 2.302585\n",
      "iteration 14800 / 20000: loss 2.302585\n",
      "iteration 14900 / 20000: loss 2.302585\n",
      "iteration 15000 / 20000: loss 2.302585\n",
      "iteration 15100 / 20000: loss 2.302585\n",
      "iteration 15200 / 20000: loss 2.302585\n",
      "iteration 15300 / 20000: loss 2.302585\n",
      "iteration 15400 / 20000: loss 2.302585\n",
      "iteration 15500 / 20000: loss 2.302585\n",
      "iteration 15600 / 20000: loss 2.302585\n",
      "iteration 15700 / 20000: loss 2.302585\n",
      "iteration 15800 / 20000: loss 2.302585\n",
      "iteration 15900 / 20000: loss 2.302585\n",
      "iteration 16000 / 20000: loss 2.302585\n",
      "iteration 16100 / 20000: loss 2.302585\n",
      "iteration 16200 / 20000: loss 2.302585\n",
      "iteration 16300 / 20000: loss 2.302585\n",
      "iteration 16400 / 20000: loss 2.302585\n",
      "iteration 16500 / 20000: loss 2.302585\n",
      "iteration 16600 / 20000: loss 2.302585\n",
      "iteration 16700 / 20000: loss 2.302585\n",
      "iteration 16800 / 20000: loss 2.302585\n",
      "iteration 16900 / 20000: loss 2.302585\n",
      "iteration 17000 / 20000: loss 2.302585\n",
      "iteration 17100 / 20000: loss 2.302585\n",
      "iteration 17200 / 20000: loss 2.302585\n",
      "iteration 17300 / 20000: loss 2.302585\n",
      "iteration 17400 / 20000: loss 2.302585\n",
      "iteration 17500 / 20000: loss 2.302585\n",
      "iteration 17600 / 20000: loss 2.302585\n",
      "iteration 17700 / 20000: loss 2.302585\n",
      "iteration 17800 / 20000: loss 2.302585\n",
      "iteration 17900 / 20000: loss 2.302585\n",
      "iteration 18000 / 20000: loss 2.302585\n",
      "iteration 18100 / 20000: loss 2.302585\n",
      "iteration 18200 / 20000: loss 2.302585\n",
      "iteration 18300 / 20000: loss 2.302585\n",
      "iteration 18400 / 20000: loss 2.302585\n",
      "iteration 18500 / 20000: loss 2.302585\n",
      "iteration 18600 / 20000: loss 2.302585\n",
      "iteration 18700 / 20000: loss 2.302585\n",
      "iteration 18800 / 20000: loss 2.302585\n",
      "iteration 18900 / 20000: loss 2.302585\n",
      "iteration 19000 / 20000: loss 2.302585\n",
      "iteration 19100 / 20000: loss 2.302585\n",
      "iteration 19200 / 20000: loss 2.302585\n",
      "iteration 19300 / 20000: loss 2.302585\n",
      "iteration 19400 / 20000: loss 2.302585\n",
      "iteration 19500 / 20000: loss 2.302585\n",
      "iteration 19600 / 20000: loss 2.302585\n",
      "iteration 19700 / 20000: loss 2.302585\n",
      "iteration 19800 / 20000: loss 2.302585\n",
      "iteration 19900 / 20000: loss 2.302585\n",
      "training accuracy: 0.412646\n",
      "validation accuracy: 0.420000\n",
      "iteration 0 / 20000: loss 79.480149\n",
      "iteration 100 / 20000: loss 65.482495\n",
      "iteration 200 / 20000: loss 54.024945\n",
      "iteration 300 / 20000: loss 44.645432\n",
      "iteration 400 / 20000: loss 36.966234\n",
      "iteration 500 / 20000: loss 30.680622\n",
      "iteration 600 / 20000: loss 25.533786\n",
      "iteration 700 / 20000: loss 21.321242\n",
      "iteration 800 / 20000: loss 17.872205\n",
      "iteration 900 / 20000: loss 15.048342\n",
      "iteration 1000 / 20000: loss 12.736681\n",
      "iteration 1100 / 20000: loss 10.843968\n",
      "iteration 1200 / 20000: loss 9.295061\n",
      "iteration 1300 / 20000: loss 8.027322\n",
      "iteration 1400 / 20000: loss 6.989253\n",
      "iteration 1500 / 20000: loss 6.139092\n",
      "iteration 1600 / 20000: loss 5.443676\n",
      "iteration 1700 / 20000: loss 4.873791\n",
      "iteration 1800 / 20000: loss 4.407501\n",
      "iteration 1900 / 20000: loss 4.025713\n",
      "iteration 2000 / 20000: loss 3.713536\n",
      "iteration 2100 / 20000: loss 3.457595\n",
      "iteration 2200 / 20000: loss 3.247903\n",
      "iteration 2300 / 20000: loss 3.076622\n",
      "iteration 2400 / 20000: loss 2.936276\n",
      "iteration 2500 / 20000: loss 2.821257\n",
      "iteration 2600 / 20000: loss 2.727228\n",
      "iteration 2700 / 20000: loss 2.650268\n",
      "iteration 2800 / 20000: loss 2.587124\n",
      "iteration 2900 / 20000: loss 2.535547\n",
      "iteration 3000 / 20000: loss 2.493337\n",
      "iteration 3100 / 20000: loss 2.458698\n",
      "iteration 3200 / 20000: loss 2.430368\n",
      "iteration 3300 / 20000: loss 2.407215\n",
      "iteration 3400 / 20000: loss 2.388205\n",
      "iteration 3500 / 20000: loss 2.372748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3600 / 20000: loss 2.359983\n",
      "iteration 3700 / 20000: loss 2.349587\n",
      "iteration 3800 / 20000: loss 2.341052\n",
      "iteration 3900 / 20000: loss 2.334098\n",
      "iteration 4000 / 20000: loss 2.328384\n",
      "iteration 4100 / 20000: loss 2.323680\n",
      "iteration 4200 / 20000: loss 2.319882\n",
      "iteration 4300 / 20000: loss 2.316734\n",
      "iteration 4400 / 20000: loss 2.314153\n",
      "iteration 4500 / 20000: loss 2.312053\n",
      "iteration 4600 / 20000: loss 2.310330\n",
      "iteration 4700 / 20000: loss 2.308934\n",
      "iteration 4800 / 20000: loss 2.307788\n",
      "iteration 4900 / 20000: loss 2.306834\n",
      "iteration 5000 / 20000: loss 2.306072\n",
      "iteration 5100 / 20000: loss 2.305436\n",
      "iteration 5200 / 20000: loss 2.304926\n",
      "iteration 5300 / 20000: loss 2.304493\n",
      "iteration 5400 / 20000: loss 2.304148\n",
      "iteration 5500 / 20000: loss 2.303862\n",
      "iteration 5600 / 20000: loss 2.303631\n",
      "iteration 5700 / 20000: loss 2.303434\n",
      "iteration 5800 / 20000: loss 2.303286\n",
      "iteration 5900 / 20000: loss 2.303159\n",
      "iteration 6000 / 20000: loss 2.303055\n",
      "iteration 6100 / 20000: loss 2.302964\n",
      "iteration 6200 / 20000: loss 2.302897\n",
      "iteration 6300 / 20000: loss 2.302842\n",
      "iteration 6400 / 20000: loss 2.302792\n",
      "iteration 6500 / 20000: loss 2.302757\n",
      "iteration 6600 / 20000: loss 2.302724\n",
      "iteration 6700 / 20000: loss 2.302696\n",
      "iteration 6800 / 20000: loss 2.302677\n",
      "iteration 6900 / 20000: loss 2.302660\n",
      "iteration 7000 / 20000: loss 2.302644\n",
      "iteration 7100 / 20000: loss 2.302634\n",
      "iteration 7200 / 20000: loss 2.302624\n",
      "iteration 7300 / 20000: loss 2.302617\n",
      "iteration 7400 / 20000: loss 2.302611\n",
      "iteration 7500 / 20000: loss 2.302605\n",
      "iteration 7600 / 20000: loss 2.302601\n",
      "iteration 7700 / 20000: loss 2.302598\n",
      "iteration 7800 / 20000: loss 2.302595\n",
      "iteration 7900 / 20000: loss 2.302592\n",
      "iteration 8000 / 20000: loss 2.302591\n",
      "iteration 8100 / 20000: loss 2.302589\n",
      "iteration 8200 / 20000: loss 2.302588\n",
      "iteration 8300 / 20000: loss 2.302586\n",
      "iteration 8400 / 20000: loss 2.302586\n",
      "iteration 8500 / 20000: loss 2.302585\n",
      "iteration 8600 / 20000: loss 2.302584\n",
      "iteration 8700 / 20000: loss 2.302583\n",
      "iteration 8800 / 20000: loss 2.302584\n",
      "iteration 8900 / 20000: loss 2.302582\n",
      "iteration 9000 / 20000: loss 2.302583\n",
      "iteration 9100 / 20000: loss 2.302582\n",
      "iteration 9200 / 20000: loss 2.302583\n",
      "iteration 9300 / 20000: loss 2.302582\n",
      "iteration 9400 / 20000: loss 2.302582\n",
      "iteration 9500 / 20000: loss 2.302582\n",
      "iteration 9600 / 20000: loss 2.302582\n",
      "iteration 9700 / 20000: loss 2.302581\n",
      "iteration 9800 / 20000: loss 2.302582\n",
      "iteration 9900 / 20000: loss 2.302582\n",
      "iteration 10000 / 20000: loss 2.302581\n",
      "iteration 10100 / 20000: loss 2.302582\n",
      "iteration 10200 / 20000: loss 2.302582\n",
      "iteration 10300 / 20000: loss 2.302582\n",
      "iteration 10400 / 20000: loss 2.302582\n",
      "iteration 10500 / 20000: loss 2.302581\n",
      "iteration 10600 / 20000: loss 2.302581\n",
      "iteration 10700 / 20000: loss 2.302582\n",
      "iteration 10800 / 20000: loss 2.302582\n",
      "iteration 10900 / 20000: loss 2.302581\n",
      "iteration 11000 / 20000: loss 2.302582\n",
      "iteration 11100 / 20000: loss 2.302582\n",
      "iteration 11200 / 20000: loss 2.302581\n",
      "iteration 11300 / 20000: loss 2.302581\n",
      "iteration 11400 / 20000: loss 2.302582\n",
      "iteration 11500 / 20000: loss 2.302581\n",
      "iteration 11600 / 20000: loss 2.302582\n",
      "iteration 11700 / 20000: loss 2.302581\n",
      "iteration 11800 / 20000: loss 2.302582\n",
      "iteration 11900 / 20000: loss 2.302581\n",
      "iteration 12000 / 20000: loss 2.302581\n",
      "iteration 12100 / 20000: loss 2.302581\n",
      "iteration 12200 / 20000: loss 2.302581\n",
      "iteration 12300 / 20000: loss 2.302581\n",
      "iteration 12400 / 20000: loss 2.302582\n",
      "iteration 12500 / 20000: loss 2.302582\n",
      "iteration 12600 / 20000: loss 2.302582\n",
      "iteration 12700 / 20000: loss 2.302581\n",
      "iteration 12800 / 20000: loss 2.302581\n",
      "iteration 12900 / 20000: loss 2.302581\n",
      "iteration 13000 / 20000: loss 2.302582\n",
      "iteration 13100 / 20000: loss 2.302582\n",
      "iteration 13200 / 20000: loss 2.302582\n",
      "iteration 13300 / 20000: loss 2.302582\n",
      "iteration 13400 / 20000: loss 2.302582\n",
      "iteration 13500 / 20000: loss 2.302581\n",
      "iteration 13600 / 20000: loss 2.302582\n",
      "iteration 13700 / 20000: loss 2.302582\n",
      "iteration 13800 / 20000: loss 2.302582\n",
      "iteration 13900 / 20000: loss 2.302582\n",
      "iteration 14000 / 20000: loss 2.302581\n",
      "iteration 14100 / 20000: loss 2.302582\n",
      "iteration 14200 / 20000: loss 2.302582\n",
      "iteration 14300 / 20000: loss 2.302581\n",
      "iteration 14400 / 20000: loss 2.302582\n",
      "iteration 14500 / 20000: loss 2.302581\n",
      "iteration 14600 / 20000: loss 2.302582\n",
      "iteration 14700 / 20000: loss 2.302583\n",
      "iteration 14800 / 20000: loss 2.302582\n",
      "iteration 14900 / 20000: loss 2.302581\n",
      "iteration 15000 / 20000: loss 2.302582\n",
      "iteration 15100 / 20000: loss 2.302581\n",
      "iteration 15200 / 20000: loss 2.302582\n",
      "iteration 15300 / 20000: loss 2.302582\n",
      "iteration 15400 / 20000: loss 2.302582\n",
      "iteration 15500 / 20000: loss 2.302582\n",
      "iteration 15600 / 20000: loss 2.302582\n",
      "iteration 15700 / 20000: loss 2.302581\n",
      "iteration 15800 / 20000: loss 2.302582\n",
      "iteration 15900 / 20000: loss 2.302582\n",
      "iteration 16000 / 20000: loss 2.302583\n",
      "iteration 16100 / 20000: loss 2.302582\n",
      "iteration 16200 / 20000: loss 2.302582\n",
      "iteration 16300 / 20000: loss 2.302581\n",
      "iteration 16400 / 20000: loss 2.302582\n",
      "iteration 16500 / 20000: loss 2.302581\n",
      "iteration 16600 / 20000: loss 2.302582\n",
      "iteration 16700 / 20000: loss 2.302582\n",
      "iteration 16800 / 20000: loss 2.302582\n",
      "iteration 16900 / 20000: loss 2.302582\n",
      "iteration 17000 / 20000: loss 2.302581\n",
      "iteration 17100 / 20000: loss 2.302582\n",
      "iteration 17200 / 20000: loss 2.302582\n",
      "iteration 17300 / 20000: loss 2.302581\n",
      "iteration 17400 / 20000: loss 2.302581\n",
      "iteration 17500 / 20000: loss 2.302581\n",
      "iteration 17600 / 20000: loss 2.302581\n",
      "iteration 17700 / 20000: loss 2.302582\n",
      "iteration 17800 / 20000: loss 2.302581\n",
      "iteration 17900 / 20000: loss 2.302582\n",
      "iteration 18000 / 20000: loss 2.302581\n",
      "iteration 18100 / 20000: loss 2.302581\n",
      "iteration 18200 / 20000: loss 2.302582\n",
      "iteration 18300 / 20000: loss 2.302581\n",
      "iteration 18400 / 20000: loss 2.302582\n",
      "iteration 18500 / 20000: loss 2.302581\n",
      "iteration 18600 / 20000: loss 2.302581\n",
      "iteration 18700 / 20000: loss 2.302582\n",
      "iteration 18800 / 20000: loss 2.302581\n",
      "iteration 18900 / 20000: loss 2.302581\n",
      "iteration 19000 / 20000: loss 2.302582\n",
      "iteration 19100 / 20000: loss 2.302581\n",
      "iteration 19200 / 20000: loss 2.302582\n",
      "iteration 19300 / 20000: loss 2.302581\n",
      "iteration 19400 / 20000: loss 2.302582\n",
      "iteration 19500 / 20000: loss 2.302582\n",
      "iteration 19600 / 20000: loss 2.302581\n",
      "iteration 19700 / 20000: loss 2.302581\n",
      "iteration 19800 / 20000: loss 2.302581\n",
      "iteration 19900 / 20000: loss 2.302582\n",
      "training accuracy: 0.413333\n",
      "validation accuracy: 0.419000\n",
      "iteration 0 / 20000: loss 781.450636\n",
      "iteration 100 / 20000: loss 106.692630\n",
      "iteration 200 / 20000: loss 16.288933\n",
      "iteration 300 / 20000: loss 4.176474\n",
      "iteration 400 / 20000: loss 2.553655\n",
      "iteration 500 / 20000: loss 2.336219\n",
      "iteration 600 / 20000: loss 2.307094\n",
      "iteration 700 / 20000: loss 2.303188\n",
      "iteration 800 / 20000: loss 2.302666\n",
      "iteration 900 / 20000: loss 2.302596\n",
      "iteration 1000 / 20000: loss 2.302586\n",
      "iteration 1100 / 20000: loss 2.302585\n",
      "iteration 1200 / 20000: loss 2.302585\n",
      "iteration 1300 / 20000: loss 2.302585\n",
      "iteration 1400 / 20000: loss 2.302585\n",
      "iteration 1500 / 20000: loss 2.302585\n",
      "iteration 1600 / 20000: loss 2.302585\n",
      "iteration 1700 / 20000: loss 2.302585\n",
      "iteration 1800 / 20000: loss 2.302585\n",
      "iteration 1900 / 20000: loss 2.302585\n",
      "iteration 2000 / 20000: loss 2.302585\n",
      "iteration 2100 / 20000: loss 2.302585\n",
      "iteration 2200 / 20000: loss 2.302585\n",
      "iteration 2300 / 20000: loss 2.302585\n",
      "iteration 2400 / 20000: loss 2.302585\n",
      "iteration 2500 / 20000: loss 2.302585\n",
      "iteration 2600 / 20000: loss 2.302585\n",
      "iteration 2700 / 20000: loss 2.302585\n",
      "iteration 2800 / 20000: loss 2.302585\n",
      "iteration 2900 / 20000: loss 2.302585\n",
      "iteration 3000 / 20000: loss 2.302585\n",
      "iteration 3100 / 20000: loss 2.302585\n",
      "iteration 3200 / 20000: loss 2.302585\n",
      "iteration 3300 / 20000: loss 2.302585\n",
      "iteration 3400 / 20000: loss 2.302585\n",
      "iteration 3500 / 20000: loss 2.302585\n",
      "iteration 3600 / 20000: loss 2.302585\n",
      "iteration 3700 / 20000: loss 2.302585\n",
      "iteration 3800 / 20000: loss 2.302585\n",
      "iteration 3900 / 20000: loss 2.302585\n",
      "iteration 4000 / 20000: loss 2.302585\n",
      "iteration 4100 / 20000: loss 2.302585\n",
      "iteration 4200 / 20000: loss 2.302585\n",
      "iteration 4300 / 20000: loss 2.302585\n",
      "iteration 4400 / 20000: loss 2.302585\n",
      "iteration 4500 / 20000: loss 2.302585\n",
      "iteration 4600 / 20000: loss 2.302585\n",
      "iteration 4700 / 20000: loss 2.302585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4800 / 20000: loss 2.302585\n",
      "iteration 4900 / 20000: loss 2.302585\n",
      "iteration 5000 / 20000: loss 2.302585\n",
      "iteration 5100 / 20000: loss 2.302585\n",
      "iteration 5200 / 20000: loss 2.302585\n",
      "iteration 5300 / 20000: loss 2.302585\n",
      "iteration 5400 / 20000: loss 2.302585\n",
      "iteration 5500 / 20000: loss 2.302585\n",
      "iteration 5600 / 20000: loss 2.302585\n",
      "iteration 5700 / 20000: loss 2.302585\n",
      "iteration 5800 / 20000: loss 2.302585\n",
      "iteration 5900 / 20000: loss 2.302585\n",
      "iteration 6000 / 20000: loss 2.302585\n",
      "iteration 6100 / 20000: loss 2.302585\n",
      "iteration 6200 / 20000: loss 2.302585\n",
      "iteration 6300 / 20000: loss 2.302585\n",
      "iteration 6400 / 20000: loss 2.302585\n",
      "iteration 6500 / 20000: loss 2.302585\n",
      "iteration 6600 / 20000: loss 2.302585\n",
      "iteration 6700 / 20000: loss 2.302585\n",
      "iteration 6800 / 20000: loss 2.302585\n",
      "iteration 6900 / 20000: loss 2.302585\n",
      "iteration 7000 / 20000: loss 2.302585\n",
      "iteration 7100 / 20000: loss 2.302585\n",
      "iteration 7200 / 20000: loss 2.302585\n",
      "iteration 7300 / 20000: loss 2.302585\n",
      "iteration 7400 / 20000: loss 2.302585\n",
      "iteration 7500 / 20000: loss 2.302585\n",
      "iteration 7600 / 20000: loss 2.302585\n",
      "iteration 7700 / 20000: loss 2.302585\n",
      "iteration 7800 / 20000: loss 2.302585\n",
      "iteration 7900 / 20000: loss 2.302585\n",
      "iteration 8000 / 20000: loss 2.302585\n",
      "iteration 8100 / 20000: loss 2.302585\n",
      "iteration 8200 / 20000: loss 2.302585\n",
      "iteration 8300 / 20000: loss 2.302585\n",
      "iteration 8400 / 20000: loss 2.302585\n",
      "iteration 8500 / 20000: loss 2.302585\n",
      "iteration 8600 / 20000: loss 2.302585\n",
      "iteration 8700 / 20000: loss 2.302585\n",
      "iteration 8800 / 20000: loss 2.302585\n",
      "iteration 8900 / 20000: loss 2.302585\n",
      "iteration 9000 / 20000: loss 2.302585\n",
      "iteration 9100 / 20000: loss 2.302585\n",
      "iteration 9200 / 20000: loss 2.302585\n",
      "iteration 9300 / 20000: loss 2.302585\n",
      "iteration 9400 / 20000: loss 2.302585\n",
      "iteration 9500 / 20000: loss 2.302585\n",
      "iteration 9600 / 20000: loss 2.302585\n",
      "iteration 9700 / 20000: loss 2.302585\n",
      "iteration 9800 / 20000: loss 2.302585\n",
      "iteration 9900 / 20000: loss 2.302585\n",
      "iteration 10000 / 20000: loss 2.302585\n",
      "iteration 10100 / 20000: loss 2.302585\n",
      "iteration 10200 / 20000: loss 2.302585\n",
      "iteration 10300 / 20000: loss 2.302585\n",
      "iteration 10400 / 20000: loss 2.302585\n",
      "iteration 10500 / 20000: loss 2.302585\n",
      "iteration 10600 / 20000: loss 2.302585\n",
      "iteration 10700 / 20000: loss 2.302585\n",
      "iteration 10800 / 20000: loss 2.302585\n",
      "iteration 10900 / 20000: loss 2.302585\n",
      "iteration 11000 / 20000: loss 2.302585\n",
      "iteration 11100 / 20000: loss 2.302585\n",
      "iteration 11200 / 20000: loss 2.302585\n",
      "iteration 11300 / 20000: loss 2.302585\n",
      "iteration 11400 / 20000: loss 2.302585\n",
      "iteration 11500 / 20000: loss 2.302585\n",
      "iteration 11600 / 20000: loss 2.302585\n",
      "iteration 11700 / 20000: loss 2.302585\n",
      "iteration 11800 / 20000: loss 2.302585\n",
      "iteration 11900 / 20000: loss 2.302585\n",
      "iteration 12000 / 20000: loss 2.302585\n",
      "iteration 12100 / 20000: loss 2.302585\n",
      "iteration 12200 / 20000: loss 2.302585\n",
      "iteration 12300 / 20000: loss 2.302585\n",
      "iteration 12400 / 20000: loss 2.302585\n",
      "iteration 12500 / 20000: loss 2.302585\n",
      "iteration 12600 / 20000: loss 2.302585\n",
      "iteration 12700 / 20000: loss 2.302585\n",
      "iteration 12800 / 20000: loss 2.302585\n",
      "iteration 12900 / 20000: loss 2.302585\n",
      "iteration 13000 / 20000: loss 2.302585\n",
      "iteration 13100 / 20000: loss 2.302585\n",
      "iteration 13200 / 20000: loss 2.302585\n",
      "iteration 13300 / 20000: loss 2.302585\n",
      "iteration 13400 / 20000: loss 2.302585\n",
      "iteration 13500 / 20000: loss 2.302585\n",
      "iteration 13600 / 20000: loss 2.302585\n",
      "iteration 13700 / 20000: loss 2.302585\n",
      "iteration 13800 / 20000: loss 2.302585\n",
      "iteration 13900 / 20000: loss 2.302585\n",
      "iteration 14000 / 20000: loss 2.302585\n",
      "iteration 14100 / 20000: loss 2.302585\n",
      "iteration 14200 / 20000: loss 2.302585\n",
      "iteration 14300 / 20000: loss 2.302585\n",
      "iteration 14400 / 20000: loss 2.302585\n",
      "iteration 14500 / 20000: loss 2.302585\n",
      "iteration 14600 / 20000: loss 2.302585\n",
      "iteration 14700 / 20000: loss 2.302585\n",
      "iteration 14800 / 20000: loss 2.302585\n",
      "iteration 14900 / 20000: loss 2.302585\n",
      "iteration 15000 / 20000: loss 2.302585\n",
      "iteration 15100 / 20000: loss 2.302585\n",
      "iteration 15200 / 20000: loss 2.302585\n",
      "iteration 15300 / 20000: loss 2.302585\n",
      "iteration 15400 / 20000: loss 2.302585\n",
      "iteration 15500 / 20000: loss 2.302585\n",
      "iteration 15600 / 20000: loss 2.302585\n",
      "iteration 15700 / 20000: loss 2.302585\n",
      "iteration 15800 / 20000: loss 2.302585\n",
      "iteration 15900 / 20000: loss 2.302585\n",
      "iteration 16000 / 20000: loss 2.302585\n",
      "iteration 16100 / 20000: loss 2.302585\n",
      "iteration 16200 / 20000: loss 2.302585\n",
      "iteration 16300 / 20000: loss 2.302585\n",
      "iteration 16400 / 20000: loss 2.302585\n",
      "iteration 16500 / 20000: loss 2.302585\n",
      "iteration 16600 / 20000: loss 2.302585\n",
      "iteration 16700 / 20000: loss 2.302585\n",
      "iteration 16800 / 20000: loss 2.302585\n",
      "iteration 16900 / 20000: loss 2.302585\n",
      "iteration 17000 / 20000: loss 2.302585\n",
      "iteration 17100 / 20000: loss 2.302585\n",
      "iteration 17200 / 20000: loss 2.302585\n",
      "iteration 17300 / 20000: loss 2.302585\n",
      "iteration 17400 / 20000: loss 2.302585\n",
      "iteration 17500 / 20000: loss 2.302585\n",
      "iteration 17600 / 20000: loss 2.302585\n",
      "iteration 17700 / 20000: loss 2.302585\n",
      "iteration 17800 / 20000: loss 2.302585\n",
      "iteration 17900 / 20000: loss 2.302585\n",
      "iteration 18000 / 20000: loss 2.302585\n",
      "iteration 18100 / 20000: loss 2.302585\n",
      "iteration 18200 / 20000: loss 2.302585\n",
      "iteration 18300 / 20000: loss 2.302585\n",
      "iteration 18400 / 20000: loss 2.302585\n",
      "iteration 18500 / 20000: loss 2.302585\n",
      "iteration 18600 / 20000: loss 2.302585\n",
      "iteration 18700 / 20000: loss 2.302585\n",
      "iteration 18800 / 20000: loss 2.302585\n",
      "iteration 18900 / 20000: loss 2.302585\n",
      "iteration 19000 / 20000: loss 2.302585\n",
      "iteration 19100 / 20000: loss 2.302585\n",
      "iteration 19200 / 20000: loss 2.302585\n",
      "iteration 19300 / 20000: loss 2.302585\n",
      "iteration 19400 / 20000: loss 2.302585\n",
      "iteration 19500 / 20000: loss 2.302585\n",
      "iteration 19600 / 20000: loss 2.302585\n",
      "iteration 19700 / 20000: loss 2.302585\n",
      "iteration 19800 / 20000: loss 2.302585\n",
      "iteration 19900 / 20000: loss 2.302585\n",
      "training accuracy: 0.413521\n",
      "validation accuracy: 0.420000\n",
      "iteration 0 / 20000: loss 7742.646079\n",
      "iteration 100 / 20000: loss 2.302591\n",
      "iteration 200 / 20000: loss 2.302585\n",
      "iteration 300 / 20000: loss 2.302585\n",
      "iteration 400 / 20000: loss 2.302585\n",
      "iteration 500 / 20000: loss 2.302585\n",
      "iteration 600 / 20000: loss 2.302585\n",
      "iteration 700 / 20000: loss 2.302585\n",
      "iteration 800 / 20000: loss 2.302585\n",
      "iteration 900 / 20000: loss 2.302585\n",
      "iteration 1000 / 20000: loss 2.302585\n",
      "iteration 1100 / 20000: loss 2.302585\n",
      "iteration 1200 / 20000: loss 2.302585\n",
      "iteration 1300 / 20000: loss 2.302585\n",
      "iteration 1400 / 20000: loss 2.302585\n",
      "iteration 1500 / 20000: loss 2.302585\n",
      "iteration 1600 / 20000: loss 2.302585\n",
      "iteration 1700 / 20000: loss 2.302585\n",
      "iteration 1800 / 20000: loss 2.302585\n",
      "iteration 1900 / 20000: loss 2.302585\n",
      "iteration 2000 / 20000: loss 2.302585\n",
      "iteration 2100 / 20000: loss 2.302585\n",
      "iteration 2200 / 20000: loss 2.302585\n",
      "iteration 2300 / 20000: loss 2.302585\n",
      "iteration 2400 / 20000: loss 2.302585\n",
      "iteration 2500 / 20000: loss 2.302585\n",
      "iteration 2600 / 20000: loss 2.302585\n",
      "iteration 2700 / 20000: loss 2.302585\n",
      "iteration 2800 / 20000: loss 2.302585\n",
      "iteration 2900 / 20000: loss 2.302585\n",
      "iteration 3000 / 20000: loss 2.302585\n",
      "iteration 3100 / 20000: loss 2.302585\n",
      "iteration 3200 / 20000: loss 2.302585\n",
      "iteration 3300 / 20000: loss 2.302585\n",
      "iteration 3400 / 20000: loss 2.302585\n",
      "iteration 3500 / 20000: loss 2.302585\n",
      "iteration 3600 / 20000: loss 2.302585\n",
      "iteration 3700 / 20000: loss 2.302585\n",
      "iteration 3800 / 20000: loss 2.302585\n",
      "iteration 3900 / 20000: loss 2.302585\n",
      "iteration 4000 / 20000: loss 2.302585\n",
      "iteration 4100 / 20000: loss 2.302585\n",
      "iteration 4200 / 20000: loss 2.302585\n",
      "iteration 4300 / 20000: loss 2.302585\n",
      "iteration 4400 / 20000: loss 2.302585\n",
      "iteration 4500 / 20000: loss 2.302585\n",
      "iteration 4600 / 20000: loss 2.302585\n",
      "iteration 4700 / 20000: loss 2.302585\n",
      "iteration 4800 / 20000: loss 2.302585\n",
      "iteration 4900 / 20000: loss 2.302585\n",
      "iteration 5000 / 20000: loss 2.302585\n",
      "iteration 5100 / 20000: loss 2.302585\n",
      "iteration 5200 / 20000: loss 2.302585\n",
      "iteration 5300 / 20000: loss 2.302585\n",
      "iteration 5400 / 20000: loss 2.302585\n",
      "iteration 5500 / 20000: loss 2.302585\n",
      "iteration 5600 / 20000: loss 2.302585\n",
      "iteration 5700 / 20000: loss 2.302585\n",
      "iteration 5800 / 20000: loss 2.302585\n",
      "iteration 5900 / 20000: loss 2.302585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 6000 / 20000: loss 2.302585\n",
      "iteration 6100 / 20000: loss 2.302585\n",
      "iteration 6200 / 20000: loss 2.302585\n",
      "iteration 6300 / 20000: loss 2.302585\n",
      "iteration 6400 / 20000: loss 2.302585\n",
      "iteration 6500 / 20000: loss 2.302585\n",
      "iteration 6600 / 20000: loss 2.302585\n",
      "iteration 6700 / 20000: loss 2.302585\n",
      "iteration 6800 / 20000: loss 2.302585\n",
      "iteration 6900 / 20000: loss 2.302585\n",
      "iteration 7000 / 20000: loss 2.302585\n",
      "iteration 7100 / 20000: loss 2.302585\n",
      "iteration 7200 / 20000: loss 2.302585\n",
      "iteration 7300 / 20000: loss 2.302585\n",
      "iteration 7400 / 20000: loss 2.302585\n",
      "iteration 7500 / 20000: loss 2.302585\n",
      "iteration 7600 / 20000: loss 2.302585\n",
      "iteration 7700 / 20000: loss 2.302585\n",
      "iteration 7800 / 20000: loss 2.302585\n",
      "iteration 7900 / 20000: loss 2.302585\n",
      "iteration 8000 / 20000: loss 2.302585\n",
      "iteration 8100 / 20000: loss 2.302585\n",
      "iteration 8200 / 20000: loss 2.302585\n",
      "iteration 8300 / 20000: loss 2.302585\n",
      "iteration 8400 / 20000: loss 2.302585\n",
      "iteration 8500 / 20000: loss 2.302585\n",
      "iteration 8600 / 20000: loss 2.302585\n",
      "iteration 8700 / 20000: loss 2.302585\n",
      "iteration 8800 / 20000: loss 2.302585\n",
      "iteration 8900 / 20000: loss 2.302585\n",
      "iteration 9000 / 20000: loss 2.302585\n",
      "iteration 9100 / 20000: loss 2.302585\n",
      "iteration 9200 / 20000: loss 2.302585\n",
      "iteration 9300 / 20000: loss 2.302585\n",
      "iteration 9400 / 20000: loss 2.302585\n",
      "iteration 9500 / 20000: loss 2.302585\n",
      "iteration 9600 / 20000: loss 2.302585\n",
      "iteration 9700 / 20000: loss 2.302585\n",
      "iteration 9800 / 20000: loss 2.302585\n",
      "iteration 9900 / 20000: loss 2.302585\n",
      "iteration 10000 / 20000: loss 2.302585\n",
      "iteration 10100 / 20000: loss 2.302585\n",
      "iteration 10200 / 20000: loss 2.302585\n",
      "iteration 10300 / 20000: loss 2.302585\n",
      "iteration 10400 / 20000: loss 2.302585\n",
      "iteration 10500 / 20000: loss 2.302585\n",
      "iteration 10600 / 20000: loss 2.302585\n",
      "iteration 10700 / 20000: loss 2.302585\n",
      "iteration 10800 / 20000: loss 2.302585\n",
      "iteration 10900 / 20000: loss 2.302585\n",
      "iteration 11000 / 20000: loss 2.302585\n",
      "iteration 11100 / 20000: loss 2.302585\n",
      "iteration 11200 / 20000: loss 2.302585\n",
      "iteration 11300 / 20000: loss 2.302585\n",
      "iteration 11400 / 20000: loss 2.302585\n",
      "iteration 11500 / 20000: loss 2.302585\n",
      "iteration 11600 / 20000: loss 2.302585\n",
      "iteration 11700 / 20000: loss 2.302585\n",
      "iteration 11800 / 20000: loss 2.302585\n",
      "iteration 11900 / 20000: loss 2.302585\n",
      "iteration 12000 / 20000: loss 2.302585\n",
      "iteration 12100 / 20000: loss 2.302585\n",
      "iteration 12200 / 20000: loss 2.302585\n",
      "iteration 12300 / 20000: loss 2.302585\n",
      "iteration 12400 / 20000: loss 2.302585\n",
      "iteration 12500 / 20000: loss 2.302585\n",
      "iteration 12600 / 20000: loss 2.302585\n",
      "iteration 12700 / 20000: loss 2.302585\n",
      "iteration 12800 / 20000: loss 2.302585\n",
      "iteration 12900 / 20000: loss 2.302585\n",
      "iteration 13000 / 20000: loss 2.302585\n",
      "iteration 13100 / 20000: loss 2.302585\n",
      "iteration 13200 / 20000: loss 2.302585\n",
      "iteration 13300 / 20000: loss 2.302585\n",
      "iteration 13400 / 20000: loss 2.302585\n",
      "iteration 13500 / 20000: loss 2.302585\n",
      "iteration 13600 / 20000: loss 2.302585\n",
      "iteration 13700 / 20000: loss 2.302585\n",
      "iteration 13800 / 20000: loss 2.302585\n",
      "iteration 13900 / 20000: loss 2.302585\n",
      "iteration 14000 / 20000: loss 2.302585\n",
      "iteration 14100 / 20000: loss 2.302585\n",
      "iteration 14200 / 20000: loss 2.302585\n",
      "iteration 14300 / 20000: loss 2.302585\n",
      "iteration 14400 / 20000: loss 2.302585\n",
      "iteration 14500 / 20000: loss 2.302585\n",
      "iteration 14600 / 20000: loss 2.302585\n",
      "iteration 14700 / 20000: loss 2.302585\n",
      "iteration 14800 / 20000: loss 2.302585\n",
      "iteration 14900 / 20000: loss 2.302585\n",
      "iteration 15000 / 20000: loss 2.302585\n",
      "iteration 15100 / 20000: loss 2.302585\n",
      "iteration 15200 / 20000: loss 2.302585\n",
      "iteration 15300 / 20000: loss 2.302585\n",
      "iteration 15400 / 20000: loss 2.302585\n",
      "iteration 15500 / 20000: loss 2.302585\n",
      "iteration 15600 / 20000: loss 2.302585\n",
      "iteration 15700 / 20000: loss 2.302585\n",
      "iteration 15800 / 20000: loss 2.302585\n",
      "iteration 15900 / 20000: loss 2.302585\n",
      "iteration 16000 / 20000: loss 2.302585\n",
      "iteration 16100 / 20000: loss 2.302585\n",
      "iteration 16200 / 20000: loss 2.302585\n",
      "iteration 16300 / 20000: loss 2.302585\n",
      "iteration 16400 / 20000: loss 2.302585\n",
      "iteration 16500 / 20000: loss 2.302585\n",
      "iteration 16600 / 20000: loss 2.302585\n",
      "iteration 16700 / 20000: loss 2.302585\n",
      "iteration 16800 / 20000: loss 2.302585\n",
      "iteration 16900 / 20000: loss 2.302585\n",
      "iteration 17000 / 20000: loss 2.302585\n",
      "iteration 17100 / 20000: loss 2.302585\n",
      "iteration 17200 / 20000: loss 2.302585\n",
      "iteration 17300 / 20000: loss 2.302585\n",
      "iteration 17400 / 20000: loss 2.302585\n",
      "iteration 17500 / 20000: loss 2.302585\n",
      "iteration 17600 / 20000: loss 2.302585\n",
      "iteration 17700 / 20000: loss 2.302585\n",
      "iteration 17800 / 20000: loss 2.302585\n",
      "iteration 17900 / 20000: loss 2.302585\n",
      "iteration 18000 / 20000: loss 2.302585\n",
      "iteration 18100 / 20000: loss 2.302585\n",
      "iteration 18200 / 20000: loss 2.302585\n",
      "iteration 18300 / 20000: loss 2.302585\n",
      "iteration 18400 / 20000: loss 2.302585\n",
      "iteration 18500 / 20000: loss 2.302585\n",
      "iteration 18600 / 20000: loss 2.302585\n",
      "iteration 18700 / 20000: loss 2.302585\n",
      "iteration 18800 / 20000: loss 2.302585\n",
      "iteration 18900 / 20000: loss 2.302585\n",
      "iteration 19000 / 20000: loss 2.302585\n",
      "iteration 19100 / 20000: loss 2.302585\n",
      "iteration 19200 / 20000: loss 2.302585\n",
      "iteration 19300 / 20000: loss 2.302585\n",
      "iteration 19400 / 20000: loss 2.302585\n",
      "iteration 19500 / 20000: loss 2.302585\n",
      "iteration 19600 / 20000: loss 2.302585\n",
      "iteration 19700 / 20000: loss 2.302585\n",
      "iteration 19800 / 20000: loss 2.302585\n",
      "iteration 19900 / 20000: loss 2.302585\n",
      "training accuracy: 0.401708\n",
      "validation accuracy: 0.404000\n",
      "iteration 0 / 20000: loss 78.663658\n",
      "iteration 100 / 20000: loss 12.533271\n",
      "iteration 200 / 20000: loss 3.673341\n",
      "iteration 300 / 20000: loss 2.486224\n",
      "iteration 400 / 20000: loss 2.327187\n",
      "iteration 500 / 20000: loss 2.305874\n",
      "iteration 600 / 20000: loss 2.303019\n",
      "iteration 700 / 20000: loss 2.302643\n",
      "iteration 800 / 20000: loss 2.302589\n",
      "iteration 900 / 20000: loss 2.302583\n",
      "iteration 1000 / 20000: loss 2.302582\n",
      "iteration 1100 / 20000: loss 2.302582\n",
      "iteration 1200 / 20000: loss 2.302582\n",
      "iteration 1300 / 20000: loss 2.302581\n",
      "iteration 1400 / 20000: loss 2.302583\n",
      "iteration 1500 / 20000: loss 2.302582\n",
      "iteration 1600 / 20000: loss 2.302581\n",
      "iteration 1700 / 20000: loss 2.302582\n",
      "iteration 1800 / 20000: loss 2.302581\n",
      "iteration 1900 / 20000: loss 2.302581\n",
      "iteration 2000 / 20000: loss 2.302582\n",
      "iteration 2100 / 20000: loss 2.302582\n",
      "iteration 2200 / 20000: loss 2.302582\n",
      "iteration 2300 / 20000: loss 2.302581\n",
      "iteration 2400 / 20000: loss 2.302582\n",
      "iteration 2500 / 20000: loss 2.302581\n",
      "iteration 2600 / 20000: loss 2.302582\n",
      "iteration 2700 / 20000: loss 2.302582\n",
      "iteration 2800 / 20000: loss 2.302581\n",
      "iteration 2900 / 20000: loss 2.302582\n",
      "iteration 3000 / 20000: loss 2.302581\n",
      "iteration 3100 / 20000: loss 2.302582\n",
      "iteration 3200 / 20000: loss 2.302581\n",
      "iteration 3300 / 20000: loss 2.302581\n",
      "iteration 3400 / 20000: loss 2.302582\n",
      "iteration 3500 / 20000: loss 2.302582\n",
      "iteration 3600 / 20000: loss 2.302581\n",
      "iteration 3700 / 20000: loss 2.302582\n",
      "iteration 3800 / 20000: loss 2.302582\n",
      "iteration 3900 / 20000: loss 2.302582\n",
      "iteration 4000 / 20000: loss 2.302581\n",
      "iteration 4100 / 20000: loss 2.302582\n",
      "iteration 4200 / 20000: loss 2.302581\n",
      "iteration 4300 / 20000: loss 2.302582\n",
      "iteration 4400 / 20000: loss 2.302582\n",
      "iteration 4500 / 20000: loss 2.302582\n",
      "iteration 4600 / 20000: loss 2.302582\n",
      "iteration 4700 / 20000: loss 2.302581\n",
      "iteration 4800 / 20000: loss 2.302581\n",
      "iteration 4900 / 20000: loss 2.302581\n",
      "iteration 5000 / 20000: loss 2.302581\n",
      "iteration 5100 / 20000: loss 2.302581\n",
      "iteration 5200 / 20000: loss 2.302582\n",
      "iteration 5300 / 20000: loss 2.302582\n",
      "iteration 5400 / 20000: loss 2.302581\n",
      "iteration 5500 / 20000: loss 2.302581\n",
      "iteration 5600 / 20000: loss 2.302582\n",
      "iteration 5700 / 20000: loss 2.302582\n",
      "iteration 5800 / 20000: loss 2.302582\n",
      "iteration 5900 / 20000: loss 2.302582\n",
      "iteration 6000 / 20000: loss 2.302580\n",
      "iteration 6100 / 20000: loss 2.302582\n",
      "iteration 6200 / 20000: loss 2.302581\n",
      "iteration 6300 / 20000: loss 2.302582\n",
      "iteration 6400 / 20000: loss 2.302582\n",
      "iteration 6500 / 20000: loss 2.302581\n",
      "iteration 6600 / 20000: loss 2.302581\n",
      "iteration 6700 / 20000: loss 2.302582\n",
      "iteration 6800 / 20000: loss 2.302582\n",
      "iteration 6900 / 20000: loss 2.302581\n",
      "iteration 7000 / 20000: loss 2.302582\n",
      "iteration 7100 / 20000: loss 2.302582\n",
      "iteration 7200 / 20000: loss 2.302581\n",
      "iteration 7300 / 20000: loss 2.302581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 7400 / 20000: loss 2.302581\n",
      "iteration 7500 / 20000: loss 2.302581\n",
      "iteration 7600 / 20000: loss 2.302582\n",
      "iteration 7700 / 20000: loss 2.302582\n",
      "iteration 7800 / 20000: loss 2.302581\n",
      "iteration 7900 / 20000: loss 2.302582\n",
      "iteration 8000 / 20000: loss 2.302581\n",
      "iteration 8100 / 20000: loss 2.302582\n",
      "iteration 8200 / 20000: loss 2.302581\n",
      "iteration 8300 / 20000: loss 2.302582\n",
      "iteration 8400 / 20000: loss 2.302582\n",
      "iteration 8500 / 20000: loss 2.302581\n",
      "iteration 8600 / 20000: loss 2.302582\n",
      "iteration 8700 / 20000: loss 2.302582\n",
      "iteration 8800 / 20000: loss 2.302582\n",
      "iteration 8900 / 20000: loss 2.302581\n",
      "iteration 9000 / 20000: loss 2.302581\n",
      "iteration 9100 / 20000: loss 2.302581\n",
      "iteration 9200 / 20000: loss 2.302581\n",
      "iteration 9300 / 20000: loss 2.302581\n",
      "iteration 9400 / 20000: loss 2.302583\n",
      "iteration 9500 / 20000: loss 2.302581\n",
      "iteration 9600 / 20000: loss 2.302581\n",
      "iteration 9700 / 20000: loss 2.302582\n",
      "iteration 9800 / 20000: loss 2.302581\n",
      "iteration 9900 / 20000: loss 2.302581\n",
      "iteration 10000 / 20000: loss 2.302581\n",
      "iteration 10100 / 20000: loss 2.302582\n",
      "iteration 10200 / 20000: loss 2.302581\n",
      "iteration 10300 / 20000: loss 2.302582\n",
      "iteration 10400 / 20000: loss 2.302582\n",
      "iteration 10500 / 20000: loss 2.302580\n",
      "iteration 10600 / 20000: loss 2.302582\n",
      "iteration 10700 / 20000: loss 2.302582\n",
      "iteration 10800 / 20000: loss 2.302582\n",
      "iteration 10900 / 20000: loss 2.302580\n",
      "iteration 11000 / 20000: loss 2.302582\n",
      "iteration 11100 / 20000: loss 2.302582\n",
      "iteration 11200 / 20000: loss 2.302582\n",
      "iteration 11300 / 20000: loss 2.302581\n",
      "iteration 11400 / 20000: loss 2.302582\n",
      "iteration 11500 / 20000: loss 2.302582\n",
      "iteration 11600 / 20000: loss 2.302581\n",
      "iteration 11700 / 20000: loss 2.302581\n",
      "iteration 11800 / 20000: loss 2.302582\n",
      "iteration 11900 / 20000: loss 2.302582\n",
      "iteration 12000 / 20000: loss 2.302581\n",
      "iteration 12100 / 20000: loss 2.302582\n",
      "iteration 12200 / 20000: loss 2.302582\n",
      "iteration 12300 / 20000: loss 2.302582\n",
      "iteration 12400 / 20000: loss 2.302581\n",
      "iteration 12500 / 20000: loss 2.302581\n",
      "iteration 12600 / 20000: loss 2.302582\n",
      "iteration 12700 / 20000: loss 2.302581\n",
      "iteration 12800 / 20000: loss 2.302581\n",
      "iteration 12900 / 20000: loss 2.302581\n",
      "iteration 13000 / 20000: loss 2.302581\n",
      "iteration 13100 / 20000: loss 2.302582\n",
      "iteration 13200 / 20000: loss 2.302582\n",
      "iteration 13300 / 20000: loss 2.302582\n",
      "iteration 13400 / 20000: loss 2.302582\n",
      "iteration 13500 / 20000: loss 2.302582\n",
      "iteration 13600 / 20000: loss 2.302582\n",
      "iteration 13700 / 20000: loss 2.302581\n",
      "iteration 13800 / 20000: loss 2.302582\n",
      "iteration 13900 / 20000: loss 2.302582\n",
      "iteration 14000 / 20000: loss 2.302582\n",
      "iteration 14100 / 20000: loss 2.302582\n",
      "iteration 14200 / 20000: loss 2.302582\n",
      "iteration 14300 / 20000: loss 2.302581\n",
      "iteration 14400 / 20000: loss 2.302581\n",
      "iteration 14500 / 20000: loss 2.302581\n",
      "iteration 14600 / 20000: loss 2.302583\n",
      "iteration 14700 / 20000: loss 2.302581\n",
      "iteration 14800 / 20000: loss 2.302582\n",
      "iteration 14900 / 20000: loss 2.302581\n",
      "iteration 15000 / 20000: loss 2.302582\n",
      "iteration 15100 / 20000: loss 2.302582\n",
      "iteration 15200 / 20000: loss 2.302582\n",
      "iteration 15300 / 20000: loss 2.302582\n",
      "iteration 15400 / 20000: loss 2.302582\n",
      "iteration 15500 / 20000: loss 2.302582\n",
      "iteration 15600 / 20000: loss 2.302581\n",
      "iteration 15700 / 20000: loss 2.302582\n",
      "iteration 15800 / 20000: loss 2.302582\n",
      "iteration 15900 / 20000: loss 2.302581\n",
      "iteration 16000 / 20000: loss 2.302582\n",
      "iteration 16100 / 20000: loss 2.302581\n",
      "iteration 16200 / 20000: loss 2.302581\n",
      "iteration 16300 / 20000: loss 2.302582\n",
      "iteration 16400 / 20000: loss 2.302582\n",
      "iteration 16500 / 20000: loss 2.302581\n",
      "iteration 16600 / 20000: loss 2.302581\n",
      "iteration 16700 / 20000: loss 2.302582\n",
      "iteration 16800 / 20000: loss 2.302582\n",
      "iteration 16900 / 20000: loss 2.302582\n",
      "iteration 17000 / 20000: loss 2.302582\n",
      "iteration 17100 / 20000: loss 2.302582\n",
      "iteration 17200 / 20000: loss 2.302581\n",
      "iteration 17300 / 20000: loss 2.302582\n",
      "iteration 17400 / 20000: loss 2.302582\n",
      "iteration 17500 / 20000: loss 2.302581\n",
      "iteration 17600 / 20000: loss 2.302581\n",
      "iteration 17700 / 20000: loss 2.302582\n",
      "iteration 17800 / 20000: loss 2.302581\n",
      "iteration 17900 / 20000: loss 2.302581\n",
      "iteration 18000 / 20000: loss 2.302582\n",
      "iteration 18100 / 20000: loss 2.302582\n",
      "iteration 18200 / 20000: loss 2.302582\n",
      "iteration 18300 / 20000: loss 2.302582\n",
      "iteration 18400 / 20000: loss 2.302581\n",
      "iteration 18500 / 20000: loss 2.302582\n",
      "iteration 18600 / 20000: loss 2.302581\n",
      "iteration 18700 / 20000: loss 2.302581\n",
      "iteration 18800 / 20000: loss 2.302581\n",
      "iteration 18900 / 20000: loss 2.302581\n",
      "iteration 19000 / 20000: loss 2.302581\n",
      "iteration 19100 / 20000: loss 2.302582\n",
      "iteration 19200 / 20000: loss 2.302582\n",
      "iteration 19300 / 20000: loss 2.302581\n",
      "iteration 19400 / 20000: loss 2.302581\n",
      "iteration 19500 / 20000: loss 2.302582\n",
      "iteration 19600 / 20000: loss 2.302581\n",
      "iteration 19700 / 20000: loss 2.302582\n",
      "iteration 19800 / 20000: loss 2.302581\n",
      "iteration 19900 / 20000: loss 2.302581\n",
      "training accuracy: 0.411417\n",
      "validation accuracy: 0.407000\n",
      "iteration 0 / 20000: loss 779.701228\n",
      "iteration 100 / 20000: loss 2.302585\n",
      "iteration 200 / 20000: loss 2.302585\n",
      "iteration 300 / 20000: loss 2.302585\n",
      "iteration 400 / 20000: loss 2.302585\n",
      "iteration 500 / 20000: loss 2.302585\n",
      "iteration 600 / 20000: loss 2.302585\n",
      "iteration 700 / 20000: loss 2.302585\n",
      "iteration 800 / 20000: loss 2.302585\n",
      "iteration 900 / 20000: loss 2.302585\n",
      "iteration 1000 / 20000: loss 2.302585\n",
      "iteration 1100 / 20000: loss 2.302585\n",
      "iteration 1200 / 20000: loss 2.302585\n",
      "iteration 1300 / 20000: loss 2.302585\n",
      "iteration 1400 / 20000: loss 2.302585\n",
      "iteration 1500 / 20000: loss 2.302585\n",
      "iteration 1600 / 20000: loss 2.302585\n",
      "iteration 1700 / 20000: loss 2.302585\n",
      "iteration 1800 / 20000: loss 2.302585\n",
      "iteration 1900 / 20000: loss 2.302585\n",
      "iteration 2000 / 20000: loss 2.302585\n",
      "iteration 2100 / 20000: loss 2.302585\n",
      "iteration 2200 / 20000: loss 2.302585\n",
      "iteration 2300 / 20000: loss 2.302585\n",
      "iteration 2400 / 20000: loss 2.302585\n",
      "iteration 2500 / 20000: loss 2.302585\n",
      "iteration 2600 / 20000: loss 2.302585\n",
      "iteration 2700 / 20000: loss 2.302585\n",
      "iteration 2800 / 20000: loss 2.302585\n",
      "iteration 2900 / 20000: loss 2.302585\n",
      "iteration 3000 / 20000: loss 2.302585\n",
      "iteration 3100 / 20000: loss 2.302585\n",
      "iteration 3200 / 20000: loss 2.302585\n",
      "iteration 3300 / 20000: loss 2.302585\n",
      "iteration 3400 / 20000: loss 2.302585\n",
      "iteration 3500 / 20000: loss 2.302585\n",
      "iteration 3600 / 20000: loss 2.302585\n",
      "iteration 3700 / 20000: loss 2.302585\n",
      "iteration 3800 / 20000: loss 2.302585\n",
      "iteration 3900 / 20000: loss 2.302585\n",
      "iteration 4000 / 20000: loss 2.302585\n",
      "iteration 4100 / 20000: loss 2.302585\n",
      "iteration 4200 / 20000: loss 2.302585\n",
      "iteration 4300 / 20000: loss 2.302585\n",
      "iteration 4400 / 20000: loss 2.302585\n",
      "iteration 4500 / 20000: loss 2.302585\n",
      "iteration 4600 / 20000: loss 2.302585\n",
      "iteration 4700 / 20000: loss 2.302585\n",
      "iteration 4800 / 20000: loss 2.302585\n",
      "iteration 4900 / 20000: loss 2.302585\n",
      "iteration 5000 / 20000: loss 2.302585\n",
      "iteration 5100 / 20000: loss 2.302585\n",
      "iteration 5200 / 20000: loss 2.302585\n",
      "iteration 5300 / 20000: loss 2.302585\n",
      "iteration 5400 / 20000: loss 2.302585\n",
      "iteration 5500 / 20000: loss 2.302585\n",
      "iteration 5600 / 20000: loss 2.302585\n",
      "iteration 5700 / 20000: loss 2.302585\n",
      "iteration 5800 / 20000: loss 2.302585\n",
      "iteration 5900 / 20000: loss 2.302585\n",
      "iteration 6000 / 20000: loss 2.302585\n",
      "iteration 6100 / 20000: loss 2.302585\n",
      "iteration 6200 / 20000: loss 2.302585\n",
      "iteration 6300 / 20000: loss 2.302585\n",
      "iteration 6400 / 20000: loss 2.302585\n",
      "iteration 6500 / 20000: loss 2.302585\n",
      "iteration 6600 / 20000: loss 2.302585\n",
      "iteration 6700 / 20000: loss 2.302585\n",
      "iteration 6800 / 20000: loss 2.302585\n",
      "iteration 6900 / 20000: loss 2.302585\n",
      "iteration 7000 / 20000: loss 2.302585\n",
      "iteration 7100 / 20000: loss 2.302585\n",
      "iteration 7200 / 20000: loss 2.302585\n",
      "iteration 7300 / 20000: loss 2.302585\n",
      "iteration 7400 / 20000: loss 2.302585\n",
      "iteration 7500 / 20000: loss 2.302585\n",
      "iteration 7600 / 20000: loss 2.302585\n",
      "iteration 7700 / 20000: loss 2.302585\n",
      "iteration 7800 / 20000: loss 2.302585\n",
      "iteration 7900 / 20000: loss 2.302585\n",
      "iteration 8000 / 20000: loss 2.302585\n",
      "iteration 8100 / 20000: loss 2.302585\n",
      "iteration 8200 / 20000: loss 2.302585\n",
      "iteration 8300 / 20000: loss 2.302585\n",
      "iteration 8400 / 20000: loss 2.302585\n",
      "iteration 8500 / 20000: loss 2.302585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 8600 / 20000: loss 2.302585\n",
      "iteration 8700 / 20000: loss 2.302585\n",
      "iteration 8800 / 20000: loss 2.302585\n",
      "iteration 8900 / 20000: loss 2.302585\n",
      "iteration 9000 / 20000: loss 2.302585\n",
      "iteration 9100 / 20000: loss 2.302585\n",
      "iteration 9200 / 20000: loss 2.302585\n",
      "iteration 9300 / 20000: loss 2.302585\n",
      "iteration 9400 / 20000: loss 2.302585\n",
      "iteration 9500 / 20000: loss 2.302585\n",
      "iteration 9600 / 20000: loss 2.302585\n",
      "iteration 9700 / 20000: loss 2.302585\n",
      "iteration 9800 / 20000: loss 2.302585\n",
      "iteration 9900 / 20000: loss 2.302585\n",
      "iteration 10000 / 20000: loss 2.302585\n",
      "iteration 10100 / 20000: loss 2.302585\n",
      "iteration 10200 / 20000: loss 2.302585\n",
      "iteration 10300 / 20000: loss 2.302585\n",
      "iteration 10400 / 20000: loss 2.302585\n",
      "iteration 10500 / 20000: loss 2.302585\n",
      "iteration 10600 / 20000: loss 2.302585\n",
      "iteration 10700 / 20000: loss 2.302585\n",
      "iteration 10800 / 20000: loss 2.302585\n",
      "iteration 10900 / 20000: loss 2.302585\n",
      "iteration 11000 / 20000: loss 2.302585\n",
      "iteration 11100 / 20000: loss 2.302585\n",
      "iteration 11200 / 20000: loss 2.302585\n",
      "iteration 11300 / 20000: loss 2.302585\n",
      "iteration 11400 / 20000: loss 2.302585\n",
      "iteration 11500 / 20000: loss 2.302585\n",
      "iteration 11600 / 20000: loss 2.302585\n",
      "iteration 11700 / 20000: loss 2.302585\n",
      "iteration 11800 / 20000: loss 2.302585\n",
      "iteration 11900 / 20000: loss 2.302585\n",
      "iteration 12000 / 20000: loss 2.302585\n",
      "iteration 12100 / 20000: loss 2.302585\n",
      "iteration 12200 / 20000: loss 2.302585\n",
      "iteration 12300 / 20000: loss 2.302585\n",
      "iteration 12400 / 20000: loss 2.302585\n",
      "iteration 12500 / 20000: loss 2.302585\n",
      "iteration 12600 / 20000: loss 2.302585\n",
      "iteration 12700 / 20000: loss 2.302585\n",
      "iteration 12800 / 20000: loss 2.302585\n",
      "iteration 12900 / 20000: loss 2.302585\n",
      "iteration 13000 / 20000: loss 2.302585\n",
      "iteration 13100 / 20000: loss 2.302585\n",
      "iteration 13200 / 20000: loss 2.302585\n",
      "iteration 13300 / 20000: loss 2.302585\n",
      "iteration 13400 / 20000: loss 2.302585\n",
      "iteration 13500 / 20000: loss 2.302585\n",
      "iteration 13600 / 20000: loss 2.302585\n",
      "iteration 13700 / 20000: loss 2.302585\n",
      "iteration 13800 / 20000: loss 2.302585\n",
      "iteration 13900 / 20000: loss 2.302585\n",
      "iteration 14000 / 20000: loss 2.302585\n",
      "iteration 14100 / 20000: loss 2.302585\n",
      "iteration 14200 / 20000: loss 2.302585\n",
      "iteration 14300 / 20000: loss 2.302585\n",
      "iteration 14400 / 20000: loss 2.302585\n",
      "iteration 14500 / 20000: loss 2.302585\n",
      "iteration 14600 / 20000: loss 2.302585\n",
      "iteration 14700 / 20000: loss 2.302585\n",
      "iteration 14800 / 20000: loss 2.302585\n",
      "iteration 14900 / 20000: loss 2.302585\n",
      "iteration 15000 / 20000: loss 2.302585\n",
      "iteration 15100 / 20000: loss 2.302585\n",
      "iteration 15200 / 20000: loss 2.302585\n",
      "iteration 15300 / 20000: loss 2.302585\n",
      "iteration 15400 / 20000: loss 2.302585\n",
      "iteration 15500 / 20000: loss 2.302585\n",
      "iteration 15600 / 20000: loss 2.302585\n",
      "iteration 15700 / 20000: loss 2.302585\n",
      "iteration 15800 / 20000: loss 2.302585\n",
      "iteration 15900 / 20000: loss 2.302585\n",
      "iteration 16000 / 20000: loss 2.302585\n",
      "iteration 16100 / 20000: loss 2.302585\n",
      "iteration 16200 / 20000: loss 2.302585\n",
      "iteration 16300 / 20000: loss 2.302585\n",
      "iteration 16400 / 20000: loss 2.302585\n",
      "iteration 16500 / 20000: loss 2.302585\n",
      "iteration 16600 / 20000: loss 2.302585\n",
      "iteration 16700 / 20000: loss 2.302585\n",
      "iteration 16800 / 20000: loss 2.302585\n",
      "iteration 16900 / 20000: loss 2.302585\n",
      "iteration 17000 / 20000: loss 2.302585\n",
      "iteration 17100 / 20000: loss 2.302585\n",
      "iteration 17200 / 20000: loss 2.302585\n",
      "iteration 17300 / 20000: loss 2.302585\n",
      "iteration 17400 / 20000: loss 2.302585\n",
      "iteration 17500 / 20000: loss 2.302585\n",
      "iteration 17600 / 20000: loss 2.302585\n",
      "iteration 17700 / 20000: loss 2.302585\n",
      "iteration 17800 / 20000: loss 2.302585\n",
      "iteration 17900 / 20000: loss 2.302585\n",
      "iteration 18000 / 20000: loss 2.302585\n",
      "iteration 18100 / 20000: loss 2.302585\n",
      "iteration 18200 / 20000: loss 2.302585\n",
      "iteration 18300 / 20000: loss 2.302585\n",
      "iteration 18400 / 20000: loss 2.302585\n",
      "iteration 18500 / 20000: loss 2.302585\n",
      "iteration 18600 / 20000: loss 2.302585\n",
      "iteration 18700 / 20000: loss 2.302585\n",
      "iteration 18800 / 20000: loss 2.302585\n",
      "iteration 18900 / 20000: loss 2.302585\n",
      "iteration 19000 / 20000: loss 2.302585\n",
      "iteration 19100 / 20000: loss 2.302585\n",
      "iteration 19200 / 20000: loss 2.302585\n",
      "iteration 19300 / 20000: loss 2.302585\n",
      "iteration 19400 / 20000: loss 2.302585\n",
      "iteration 19500 / 20000: loss 2.302585\n",
      "iteration 19600 / 20000: loss 2.302585\n",
      "iteration 19700 / 20000: loss 2.302585\n",
      "iteration 19800 / 20000: loss 2.302585\n",
      "iteration 19900 / 20000: loss 2.302585\n",
      "training accuracy: 0.416542\n",
      "validation accuracy: 0.416000\n",
      "iteration 0 / 20000: loss 8097.064328\n",
      "iteration 100 / 20000: loss 2.302585\n",
      "iteration 200 / 20000: loss 2.302585\n",
      "iteration 300 / 20000: loss 2.302585\n",
      "iteration 400 / 20000: loss 2.302585\n",
      "iteration 500 / 20000: loss 2.302585\n",
      "iteration 600 / 20000: loss 2.302585\n",
      "iteration 700 / 20000: loss 2.302585\n",
      "iteration 800 / 20000: loss 2.302585\n",
      "iteration 900 / 20000: loss 2.302585\n",
      "iteration 1000 / 20000: loss 2.302585\n",
      "iteration 1100 / 20000: loss 2.302585\n",
      "iteration 1200 / 20000: loss 2.302585\n",
      "iteration 1300 / 20000: loss 2.302585\n",
      "iteration 1400 / 20000: loss 2.302585\n",
      "iteration 1500 / 20000: loss 2.302585\n",
      "iteration 1600 / 20000: loss 2.302585\n",
      "iteration 1700 / 20000: loss 2.302585\n",
      "iteration 1800 / 20000: loss 2.302585\n",
      "iteration 1900 / 20000: loss 2.302585\n",
      "iteration 2000 / 20000: loss 2.302585\n",
      "iteration 2100 / 20000: loss 2.302585\n",
      "iteration 2200 / 20000: loss 2.302585\n",
      "iteration 2300 / 20000: loss 2.302585\n",
      "iteration 2400 / 20000: loss 2.302585\n",
      "iteration 2500 / 20000: loss 2.302585\n",
      "iteration 2600 / 20000: loss 2.302585\n",
      "iteration 2700 / 20000: loss 2.302585\n",
      "iteration 2800 / 20000: loss 2.302585\n",
      "iteration 2900 / 20000: loss 2.302585\n",
      "iteration 3000 / 20000: loss 2.302585\n",
      "iteration 3100 / 20000: loss 2.302585\n",
      "iteration 3200 / 20000: loss 2.302585\n",
      "iteration 3300 / 20000: loss 2.302585\n",
      "iteration 3400 / 20000: loss 2.302585\n",
      "iteration 3500 / 20000: loss 2.302585\n",
      "iteration 3600 / 20000: loss 2.302585\n",
      "iteration 3700 / 20000: loss 2.302585\n",
      "iteration 3800 / 20000: loss 2.302585\n",
      "iteration 3900 / 20000: loss 2.302585\n",
      "iteration 4000 / 20000: loss 2.302585\n",
      "iteration 4100 / 20000: loss 2.302585\n",
      "iteration 4200 / 20000: loss 2.302585\n",
      "iteration 4300 / 20000: loss 2.302585\n",
      "iteration 4400 / 20000: loss 2.302585\n",
      "iteration 4500 / 20000: loss 2.302585\n",
      "iteration 4600 / 20000: loss 2.302585\n",
      "iteration 4700 / 20000: loss 2.302585\n",
      "iteration 4800 / 20000: loss 2.302585\n",
      "iteration 4900 / 20000: loss 2.302585\n",
      "iteration 5000 / 20000: loss 2.302585\n",
      "iteration 5100 / 20000: loss 2.302585\n",
      "iteration 5200 / 20000: loss 2.302585\n",
      "iteration 5300 / 20000: loss 2.302585\n",
      "iteration 5400 / 20000: loss 2.302585\n",
      "iteration 5500 / 20000: loss 2.302585\n",
      "iteration 5600 / 20000: loss 2.302585\n",
      "iteration 5700 / 20000: loss 2.302585\n",
      "iteration 5800 / 20000: loss 2.302585\n",
      "iteration 5900 / 20000: loss 2.302585\n",
      "iteration 6000 / 20000: loss 2.302585\n",
      "iteration 6100 / 20000: loss 2.302585\n",
      "iteration 6200 / 20000: loss 2.302585\n",
      "iteration 6300 / 20000: loss 2.302585\n",
      "iteration 6400 / 20000: loss 2.302585\n",
      "iteration 6500 / 20000: loss 2.302585\n",
      "iteration 6600 / 20000: loss 2.302585\n",
      "iteration 6700 / 20000: loss 2.302585\n",
      "iteration 6800 / 20000: loss 2.302585\n",
      "iteration 6900 / 20000: loss 2.302585\n",
      "iteration 7000 / 20000: loss 2.302585\n",
      "iteration 7100 / 20000: loss 2.302585\n",
      "iteration 7200 / 20000: loss 2.302585\n",
      "iteration 7300 / 20000: loss 2.302585\n",
      "iteration 7400 / 20000: loss 2.302585\n",
      "iteration 7500 / 20000: loss 2.302585\n",
      "iteration 7600 / 20000: loss 2.302585\n",
      "iteration 7700 / 20000: loss 2.302585\n",
      "iteration 7800 / 20000: loss 2.302585\n",
      "iteration 7900 / 20000: loss 2.302585\n",
      "iteration 8000 / 20000: loss 2.302585\n",
      "iteration 8100 / 20000: loss 2.302585\n",
      "iteration 8200 / 20000: loss 2.302585\n",
      "iteration 8300 / 20000: loss 2.302585\n",
      "iteration 8400 / 20000: loss 2.302585\n",
      "iteration 8500 / 20000: loss 2.302585\n",
      "iteration 8600 / 20000: loss 2.302585\n",
      "iteration 8700 / 20000: loss 2.302585\n",
      "iteration 8800 / 20000: loss 2.302585\n",
      "iteration 8900 / 20000: loss 2.302585\n",
      "iteration 9000 / 20000: loss 2.302585\n",
      "iteration 9100 / 20000: loss 2.302585\n",
      "iteration 9200 / 20000: loss 2.302585\n",
      "iteration 9300 / 20000: loss 2.302585\n",
      "iteration 9400 / 20000: loss 2.302585\n",
      "iteration 9500 / 20000: loss 2.302585\n",
      "iteration 9600 / 20000: loss 2.302585\n",
      "iteration 9700 / 20000: loss 2.302585\n",
      "iteration 9800 / 20000: loss 2.302585\n",
      "iteration 9900 / 20000: loss 2.302585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 10000 / 20000: loss 2.302585\n",
      "iteration 10100 / 20000: loss 2.302585\n",
      "iteration 10200 / 20000: loss 2.302585\n",
      "iteration 10300 / 20000: loss 2.302585\n",
      "iteration 10400 / 20000: loss 2.302585\n",
      "iteration 10500 / 20000: loss 2.302585\n",
      "iteration 10600 / 20000: loss 2.302585\n",
      "iteration 10700 / 20000: loss 2.302585\n",
      "iteration 10800 / 20000: loss 2.302585\n",
      "iteration 10900 / 20000: loss 2.302585\n",
      "iteration 11000 / 20000: loss 2.302585\n",
      "iteration 11100 / 20000: loss 2.302585\n",
      "iteration 11200 / 20000: loss 2.302585\n",
      "iteration 11300 / 20000: loss 2.302585\n",
      "iteration 11400 / 20000: loss 2.302585\n",
      "iteration 11500 / 20000: loss 2.302585\n",
      "iteration 11600 / 20000: loss 2.302585\n",
      "iteration 11700 / 20000: loss 2.302585\n",
      "iteration 11800 / 20000: loss 2.302585\n",
      "iteration 11900 / 20000: loss 2.302585\n",
      "iteration 12000 / 20000: loss 2.302585\n",
      "iteration 12100 / 20000: loss 2.302585\n",
      "iteration 12200 / 20000: loss 2.302585\n",
      "iteration 12300 / 20000: loss 2.302585\n",
      "iteration 12400 / 20000: loss 2.302585\n",
      "iteration 12500 / 20000: loss 2.302585\n",
      "iteration 12600 / 20000: loss 2.302585\n",
      "iteration 12700 / 20000: loss 2.302585\n",
      "iteration 12800 / 20000: loss 2.302585\n",
      "iteration 12900 / 20000: loss 2.302585\n",
      "iteration 13000 / 20000: loss 2.302585\n",
      "iteration 13100 / 20000: loss 2.302585\n",
      "iteration 13200 / 20000: loss 2.302585\n",
      "iteration 13300 / 20000: loss 2.302585\n",
      "iteration 13400 / 20000: loss 2.302585\n",
      "iteration 13500 / 20000: loss 2.302585\n",
      "iteration 13600 / 20000: loss 2.302585\n",
      "iteration 13700 / 20000: loss 2.302585\n",
      "iteration 13800 / 20000: loss 2.302585\n",
      "iteration 13900 / 20000: loss 2.302585\n",
      "iteration 14000 / 20000: loss 2.302585\n",
      "iteration 14100 / 20000: loss 2.302585\n",
      "iteration 14200 / 20000: loss 2.302585\n",
      "iteration 14300 / 20000: loss 2.302585\n",
      "iteration 14400 / 20000: loss 2.302585\n",
      "iteration 14500 / 20000: loss 2.302585\n",
      "iteration 14600 / 20000: loss 2.302585\n",
      "iteration 14700 / 20000: loss 2.302585\n",
      "iteration 14800 / 20000: loss 2.302585\n",
      "iteration 14900 / 20000: loss 2.302585\n",
      "iteration 15000 / 20000: loss 2.302585\n",
      "iteration 15100 / 20000: loss 2.302585\n",
      "iteration 15200 / 20000: loss 2.302585\n",
      "iteration 15300 / 20000: loss 2.302585\n",
      "iteration 15400 / 20000: loss 2.302585\n",
      "iteration 15500 / 20000: loss 2.302585\n",
      "iteration 15600 / 20000: loss 2.302585\n",
      "iteration 15700 / 20000: loss 2.302585\n",
      "iteration 15800 / 20000: loss 2.302585\n",
      "iteration 15900 / 20000: loss 2.302585\n",
      "iteration 16000 / 20000: loss 2.302585\n",
      "iteration 16100 / 20000: loss 2.302585\n",
      "iteration 16200 / 20000: loss 2.302585\n",
      "iteration 16300 / 20000: loss 2.302585\n",
      "iteration 16400 / 20000: loss 2.302585\n",
      "iteration 16500 / 20000: loss 2.302585\n",
      "iteration 16600 / 20000: loss 2.302585\n",
      "iteration 16700 / 20000: loss 2.302585\n",
      "iteration 16800 / 20000: loss 2.302585\n",
      "iteration 16900 / 20000: loss 2.302585\n",
      "iteration 17000 / 20000: loss 2.302585\n",
      "iteration 17100 / 20000: loss 2.302585\n",
      "iteration 17200 / 20000: loss 2.302585\n",
      "iteration 17300 / 20000: loss 2.302585\n",
      "iteration 17400 / 20000: loss 2.302585\n",
      "iteration 17500 / 20000: loss 2.302585\n",
      "iteration 17600 / 20000: loss 2.302585\n",
      "iteration 17700 / 20000: loss 2.302585\n",
      "iteration 17800 / 20000: loss 2.302585\n",
      "iteration 17900 / 20000: loss 2.302585\n",
      "iteration 18000 / 20000: loss 2.302585\n",
      "iteration 18100 / 20000: loss 2.302585\n",
      "iteration 18200 / 20000: loss 2.302585\n",
      "iteration 18300 / 20000: loss 2.302585\n",
      "iteration 18400 / 20000: loss 2.302585\n",
      "iteration 18500 / 20000: loss 2.302585\n",
      "iteration 18600 / 20000: loss 2.302585\n",
      "iteration 18700 / 20000: loss 2.302585\n",
      "iteration 18800 / 20000: loss 2.302585\n",
      "iteration 18900 / 20000: loss 2.302585\n",
      "iteration 19000 / 20000: loss 2.302585\n",
      "iteration 19100 / 20000: loss 2.302585\n",
      "iteration 19200 / 20000: loss 2.302585\n",
      "iteration 19300 / 20000: loss 2.302585\n",
      "iteration 19400 / 20000: loss 2.302585\n",
      "iteration 19500 / 20000: loss 2.302585\n",
      "iteration 19600 / 20000: loss 2.302585\n",
      "iteration 19700 / 20000: loss 2.302585\n",
      "iteration 19800 / 20000: loss 2.302585\n",
      "iteration 19900 / 20000: loss 2.302585\n",
      "training accuracy: 0.347313\n",
      "validation accuracy: 0.343000\n",
      "lr 1.000000e-09 reg 1.000000e+05 train accuracy: 0.110333 val accuracy: 0.108000\n",
      "lr 1.000000e-09 reg 1.000000e+06 train accuracy: 0.413813 val accuracy: 0.418000\n",
      "lr 1.000000e-09 reg 1.000000e+07 train accuracy: 0.412646 val accuracy: 0.420000\n",
      "lr 1.000000e-08 reg 1.000000e+05 train accuracy: 0.413333 val accuracy: 0.419000\n",
      "lr 1.000000e-08 reg 1.000000e+06 train accuracy: 0.413521 val accuracy: 0.420000\n",
      "lr 1.000000e-08 reg 1.000000e+07 train accuracy: 0.401708 val accuracy: 0.404000\n",
      "lr 1.000000e-07 reg 1.000000e+05 train accuracy: 0.411417 val accuracy: 0.407000\n",
      "lr 1.000000e-07 reg 1.000000e+06 train accuracy: 0.416542 val accuracy: 0.416000\n",
      "lr 1.000000e-07 reg 1.000000e+07 train accuracy: 0.347313 val accuracy: 0.343000\n",
      "best validation accuracy achieved during validation: 0.420000\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune the learning rate and regularization strength\n",
    "\n",
    "from exercise_code.classifiers.softmax import SoftmaxClassifier\n",
    "\n",
    "learning_rates = [1e-9, 1e-8, 1e-7]\n",
    "regularization_strengths = [1e5, 1e6, 1e7]\n",
    "\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the Softmax; save#\n",
    "# the best trained classifer in best_softmax. You might also want to play      #\n",
    "# with different numbers of bins in the color histogram. If you are careful    #\n",
    "# you should be able to get accuracy of near 0.44 on the validation set.       #\n",
    "################################################################################\n",
    "for learning_rate in learning_rates:\n",
    "    for reg in regularization_strengths:\n",
    "        softmax = SoftmaxClassifier()\n",
    "        loss_hist = softmax.train(X_train_feats, y_train, learning_rate=learning_rate, reg=reg,\n",
    "                      num_iters=20000, verbose=True)\n",
    "        y_train_pred = softmax.predict(X_train_feats)\n",
    "        trainingAccuracy = np.mean(y_train == y_train_pred)\n",
    "        print('training accuracy: %f' % trainingAccuracy, )\n",
    "        y_val_pred = softmax.predict(X_val_feats)\n",
    "        validationAccuracy = np.mean(y_val == y_val_pred)\n",
    "        print('validation accuracy: %f' % validationAccuracy, )\n",
    "        results[(learning_rate, reg)] = (trainingAccuracy, validationAccuracy)\n",
    "        if validationAccuracy > best_val:\n",
    "            best_val = validationAccuracy\n",
    "            best_softmax = softmax\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "\n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "          lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.408\n"
     ]
    }
   ],
   "source": [
    "# Evaluate your trained classifier on the test set\n",
    "y_test_pred = best_softmax.predict(X_test_feats)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-9fc01ba23fd8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0midxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples_per_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'uint8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'off'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# An important way to gain intuition about how an algorithm works is to\n",
    "# visualize the mistakes that it makes. In this visualization, we show examples\n",
    "# of images that are misclassified by our current system. The first column\n",
    "# shows images that our system labeled as \"plane\" but whose true label is\n",
    "# something other than \"plane\".\n",
    "\n",
    "examples_per_class = 8\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for cls, cls_name in enumerate(classes):\n",
    "    idxs = np.where((y_test != cls) & (y_test_pred == cls))[0]\n",
    "    if len(idxs) > 0:\n",
    "        idxs = np.random.choice(idxs, min(examples_per_class, len(idxs)), replace=False)\n",
    "    else:\n",
    "        idxs = []\n",
    "    for i, idx in enumerate(idxs):\n",
    "        plt.subplot(examples_per_class, len(classes), i * len(classes) + cls + 1)\n",
    "        plt.imshow(X_test[idx].astype('uint8'))\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(cls_name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Inline Question</h3>\n",
    "    <p>Describe the misclassification results that you see. Do they make sense?</p>\n",
    "    <p>**Your answer:** \n",
    "Some of them make sense also for human intuition. For example, animals on green background are classified as deer, images that show sky as plane or trucks as cars.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network on image features\n",
    "Earlier in this assigment we saw that training a two-layer neural network on raw pixels achieved better classification performance than linear classifiers on raw pixels. In this notebook we have seen that linear classifiers on image features outperform linear classifiers on raw pixels. \n",
    "\n",
    "For completeness, we should also try training a neural network on image features. This approach should outperform all previous approaches: you should easily be able to achieve over 55% classification accuracy on the test set.\n",
    "\n",
    "Note that in the neural net class we have used explicit biases. In the preprocessing of this exercise we have also added a bias dimension to the input, such that we don't have to consider an explicit bias in the linear classifier above. The neural net therefore has an additional parameter in the first layer (the additional bias term), which after all does not matter, as both bias terms can be learned. So just don't get confused by this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "iteration 0 / 5000: loss 2.302585\n",
      "iteration 100 / 5000: loss 1.455433\n",
      "iteration 200 / 5000: loss 1.344241\n",
      "iteration 300 / 5000: loss 1.312005\n",
      "iteration 400 / 5000: loss 1.293542\n",
      "iteration 500 / 5000: loss 1.178755\n",
      "iteration 600 / 5000: loss 1.179805\n",
      "iteration 700 / 5000: loss 1.140029\n",
      "iteration 800 / 5000: loss 1.014460\n",
      "iteration 900 / 5000: loss 1.097508\n",
      "iteration 1000 / 5000: loss 0.981105\n",
      "iteration 1100 / 5000: loss 1.144329\n",
      "iteration 1200 / 5000: loss 0.928556\n",
      "iteration 1300 / 5000: loss 0.996714\n",
      "iteration 1400 / 5000: loss 0.985578\n",
      "iteration 1500 / 5000: loss 1.011243\n",
      "iteration 1600 / 5000: loss 1.051108\n",
      "iteration 1700 / 5000: loss 0.983378\n",
      "iteration 1800 / 5000: loss 0.875399\n",
      "iteration 1900 / 5000: loss 0.966350\n",
      "iteration 2000 / 5000: loss 1.033467\n",
      "iteration 2100 / 5000: loss 0.866826\n",
      "iteration 2200 / 5000: loss 0.857086\n",
      "iteration 2300 / 5000: loss 0.935074\n",
      "iteration 2400 / 5000: loss 0.916194\n",
      "iteration 2500 / 5000: loss 0.811341\n",
      "iteration 2600 / 5000: loss 0.946139\n",
      "iteration 2700 / 5000: loss 0.839958\n",
      "iteration 2800 / 5000: loss 0.927941\n",
      "iteration 2900 / 5000: loss 0.901906\n",
      "iteration 3000 / 5000: loss 0.784466\n",
      "iteration 3100 / 5000: loss 0.774193\n",
      "iteration 3200 / 5000: loss 0.824631\n",
      "iteration 3300 / 5000: loss 0.809120\n",
      "iteration 3400 / 5000: loss 0.773723\n",
      "iteration 3500 / 5000: loss 0.770702\n",
      "iteration 3600 / 5000: loss 0.790806\n",
      "iteration 3700 / 5000: loss 0.817440\n",
      "iteration 3800 / 5000: loss 0.871129\n",
      "iteration 3900 / 5000: loss 0.795366\n",
      "iteration 4000 / 5000: loss 0.794500\n",
      "iteration 4100 / 5000: loss 0.743678\n",
      "iteration 4200 / 5000: loss 0.714038\n",
      "iteration 4300 / 5000: loss 0.729720\n",
      "iteration 4400 / 5000: loss 0.780909\n",
      "iteration 4500 / 5000: loss 0.797862\n",
      "iteration 4600 / 5000: loss 0.687055\n",
      "iteration 4700 / 5000: loss 0.760050\n",
      "iteration 4800 / 5000: loss 0.819855\n",
      "iteration 4900 / 5000: loss 0.696951\n",
      "Validation accuracy:  0.598\n"
     ]
    }
   ],
   "source": [
    "from exercise_code.classifiers.neural_net import TwoLayerNet\n",
    "\n",
    "input_dim = X_train_feats.shape[1]\n",
    "hidden_dim = 500\n",
    "num_classes = 10\n",
    "\n",
    "net = TwoLayerNet(input_dim, hidden_dim, num_classes)\n",
    "best_net = None\n",
    "\n",
    "################################################################################\n",
    "# TODO: Train a two-layer neural network on image features. You may want to    #\n",
    "# validate various parameters as in previous sections. Store your best   #\n",
    "# model in the best_net variable.                                              #\n",
    "################################################################################\n",
    "net = TwoLayerNet(input_dim, hidden_dim, num_classes)\n",
    "# Train the network\n",
    "stats = net.train(X_train_feats, y_train, X_val_feats, y_val,\n",
    "num_iters=5000, batch_size=256,\n",
    "learning_rate=0.9, learning_rate_decay=0.95,\n",
    "reg=1e-3, verbose=True)\n",
    "\n",
    "# Predict on the validation set\n",
    "val_acc = (net.predict(X_val_feats) == y_val).mean()\n",
    "print('Validation accuracy: ', val_acc)\n",
    "\n",
    "best_net = net\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.609\n"
     ]
    }
   ],
   "source": [
    "# Run your neural net classifier on the test set. You should be able to\n",
    "# get more than 55% accuracy.\n",
    "\n",
    "test_acc = (net.predict(X_test_feats) == y_test).mean()\n",
    "print(\"Test accuracy: \", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model\n",
    "\n",
    "When you are satisfied with your training, save the model for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.model_savers import save_feature_neural_net\n",
    "save_feature_neural_net(best_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring function\n",
    "We will score the model you have just saved based on the classification accuracy on our test dataset. The scoring function should represent the difficulty of obtaining a good test accuracy and should therefore give 0 points for worse results than random guessing, should be linear in a first regime and exponential beyond that. The onset of exponential growth depends on the problem. In that region you get twice as many points for an additional 10% accuracy.\n",
    "\n",
    "For this problem we specifically use the following scoring function:\n",
    "\n",
    "$$f(x) = \\left\\{\n",
    "\t\\begin{array}{ll}\n",
    "\t\t0  & \\mbox{if } x \\leq 0.1 \\\\\n",
    "\t\t100x & \\mbox{if } 0.1 < x < 0.59 \\\\\n",
    "        \\left(\\frac{59}{\\exp(0.59 \\ln(2)/0.1)}\\right) \\exp(x \\ln(2)/0.1) & \\mbox{if } 0.59 \\leq x \\leq 1\n",
    "\t\\end{array}\n",
    "\\right.$$\n",
    "\n",
    "The function can be plotted in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAm4AAAHwCAYAAAAfLOO9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10dca0a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from exercise_code.data_utils import scoring_function\n",
    "\n",
    "x = np.linspace(0, 1, num=1000)\n",
    "plt.plot(x, scoring_function(x, lin_exp_boundary=0.59, doubling_rate=0.1))\n",
    "plt.title('Scoring Function')\n",
    "plt.xlabel('Test Accuracy')\n",
    "plt.ylabel('Score')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
